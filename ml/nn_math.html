<!DOCTYPE html>
<html lang="en" x-data>
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Neural Networks — Math & Intuition (Pedagogical Page)</title>

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link
    href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap"
    rel="stylesheet">

  <!-- Tailwind (CDN) -->
  <script src="https://cdn.tailwindcss.com"></script>

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        tags: 'none'
      },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" defer></script>

  <!-- Alpine.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/alpinejs/3.13.3/cdn.min.js" defer></script>

  <!-- Styles -->
  <style>
    :root { --bg-grad: linear-gradient(135deg, #667eea 0%, #764ba2 100%); }
    html, body { min-height: 100%; }
    body { font-family: 'Inter', sans-serif; background: var(--bg-grad); }
    .font-mono { font-family: 'JetBrains Mono', monospace; }
    .card { background: white; border: 1px solid #e5e7eb; border-radius: 14px; box-shadow: 0 10px 30px rgba(2,6,23,.08); }
    .math-block { background: linear-gradient(135deg, #111827, #1f2937); border: 1px solid #374151; border-radius: 14px; padding: 1.25rem; color: #e5e7eb; }
    .note-box { background: linear-gradient(135deg, #ecfdf5, #d1fae5); border-left: 4px solid #10b981; border-radius: 10px; padding: .9rem 1rem; }
    .warn-box { background: linear-gradient(135deg, #fef3c7, #fde68a); border-left: 4px solid #f59e0b; border-radius: 10px; padding: .9rem 1rem; }
    .theorem-box { background: linear-gradient(135deg, #ede9fe, #ddd6fe); border: 1px solid #8b5cf6; border-radius: 12px; padding: 1rem 1.25rem; }
    .proof-box { background: linear-gradient(135deg, #f0f9ff, #e0f2fe); border-left: 4px solid #0ea5e9; border-radius: 10px; padding: .9rem 1rem; }
    .code { font-family: 'JetBrains Mono', monospace; font-size: .9rem; line-height: 1.5; }
    .neuron { width: 40px; height: 40px; border-radius: 999px; display: flex; align-items: center; justify-content: center; font-weight: 700; font-size: 12px; transition: transform .15s ease, opacity .2s ease; }
    .neuron.active { background: #10b981; color: white; }
    .neuron.dropped { background: #ef4444; color: white; opacity: .3; }
    .gradient-text { background: linear-gradient(135deg, #f8fafc, #c7d2fe); -webkit-background-clip: text; background-clip: text; color: transparent; }

    /* TOC active */
    .toc a[aria-current="true"] { color: #4338ca; font-weight: 600; }

    /* Anchor-friendly headings */
    main :is(h2,h3) { scroll-margin-top: 84px; position: relative; }
    .heading-anchor { position: absolute; left: -1.2em; opacity: 0; text-decoration: none; user-select: none; }
    main :is(h2,h3):hover .heading-anchor, main :is(h2,h3):focus-within .heading-anchor { opacity: .6; }
  </style>
</head>

<body class="text-slate-900">
  <div class="max-w-7xl mx-auto px-6 py-10">
    <!-- Header -->
    <header class="text-center mb-10">
      <h1 class="text-4xl font-extrabold tracking-tight text-white">
        <span class="gradient-text">Neural Networks — The Mathematics</span>
      </h1>
      <p class="mt-3 text-slate-100/90 max-w-3xl mx-auto">
        A pedagogical single page for deep learning classes. Intuition, clean derivations,
        interactive playgrounds, and expandable proof sketches. Built to scale as you add sections.
      </p>
    </header>

    <div class="grid lg:grid-cols-[260px,1fr] gap-6 items-start">
      <!-- Sidebar TOC -->
      <aside class="card p-4 sticky top-6 h-max toc" x-data="toc()" x-init="init()">
        <h3 class="text-sm font-semibold tracking-wide text-slate-600 uppercase">Contents</h3>
        <nav class="mt-3 text-sm space-y-2">
          <template x-for="(item, i) in items" :key="item.id">
            <a :href="'#' + item.id" @click.prevent="scrollTo(item.id)"
               class="block hover:text-indigo-600"
               :aria-current="activeId === item.id ? 'true' : 'false'">
              <span x-text="i + 1 + '. '"></span>
              <span x-text="item.title"></span>
            </a>
          </template>
        </nav>
      </aside>
<!-- Main -->
<main class="space-y-12" id="main">
  <!-- ======= Backpropagation (Chain Rule) ======= -->
  <section id="backprop">
    <h2 class="text-3xl font-bold mb-2">
      <a class="heading-anchor" href="#backprop" aria-label="Anchor">#</a>
      Backpropagation — Chain Rule
    </h2>

    <!-- Scalar chain rule -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6">
      <h3 class="text-xl font-semibold mb-2">Scalar Chain Rule (building block)</h3>
      <div class="math-block text-sm">
        If $y = f(u)$, $u = g(v)$, $v = h(x)$, then
        $$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\,\frac{\partial u}{\partial v}\,\frac{\partial v}{\partial x}.$$
        Backprop uses this right-to-left: propagate gradients $\delta = \partial\mathcal L/\partial(\cdot)$ through each function.
      </div>
      <div class="note-box text-sm mt-3">
        Think of each layer as a function; gradients flow by multiplying local derivatives evaluated at the current activations.
      </div>
    </article>

    <!-- Two-layer MLP -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6">
      <h3 class="text-xl font-semibold mb-2">Two-Layer MLP Example (vector form)</h3>
      <p class="text-sm text-slate-700">Network: input $x\in\mathbb R^{d}$, hidden width $h$, output scalar $\hat y$.</p>
      <div class="math-block text-sm">
        <p class="font-semibold">Forward:</p>
        $$a^{(1)} = W_1 x + b_1 \in \mathbb R^{h},\qquad h^{(1)} = f\!\left(a^{(1)}\right)$$
        $$a^{(2)} = {w_2}^\top h^{(1)} + b_2 \in \mathbb R,\qquad \hat y = a^{(2)}$$
        <p class="font-semibold mt-2">Loss (squared error):</p>
        $$\mathcal L = \tfrac{1}{2}(\hat y - y)^2.$$
      </div>
      <div class="math-block text-sm mt-3">
        <p class="font-semibold">Gradients (backprop):</p>
        Output layer:
        $$\delta^{(2)} \equiv \frac{\partial \mathcal L}{\partial a^{(2)}} = (\hat y - y)$$
        $$\frac{\partial \mathcal L}{\partial w_2} = \delta^{(2)}\, h^{(1)},\qquad \frac{\partial \mathcal L}{\partial b_2} = \delta^{(2)}$$
        Hidden pre-activation:
        $$\delta^{(1)} \equiv \frac{\partial \mathcal L}{\partial a^{(1)}} = \left( w_2\, \delta^{(2)} \right) \odot f'\!\left(a^{(1)}\right)$$
        First-layer parameters:
        $$\frac{\partial \mathcal L}{\partial W_1} = \delta^{(1)}\, x^\top,\qquad \frac{\partial \mathcal L}{\partial b_1} = \delta^{(1)}.$$
      </div>
    </article>

    <!-- Worked numeric mini-example -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6">
      <h3 class="text-xl font-semibold mb-2">Worked Mini Example (numbers)</h3>
      <p class="text-sm text-slate-700">Let $d=2$, $h=2$, $f$ = ReLU. One sample $(x,y)$ with</p>
      <div class="math-block text-sm">
        $$x=\begin{bmatrix}1\\-2\end{bmatrix},\quad
        W_1=\begin{bmatrix}1 & -1\\ 2 & 0.5\end{bmatrix},\quad
        b_1=\begin{bmatrix}0\\0\end{bmatrix},\quad
        w_2=\begin{bmatrix}1\\-3\end{bmatrix},\quad
        b_2=0,\quad y=2.$$
        <p class="font-semibold mt-2">Forward:</p>
        $$a^{(1)}=W_1x+b_1=\begin{bmatrix}1\cdot1+(-1)(-2)\\ 2\cdot1+0.5(-2)\end{bmatrix}=\begin{bmatrix}3\\1\end{bmatrix},\quad
        h^{(1)}=\operatorname{ReLU}(a^{(1)})=\begin{bmatrix}3\\1\end{bmatrix}$$
        $$a^{(2)}=w_2^\top h^{(1)}+b_2=1\cdot3+(-3)\cdot1=0,\quad \hat y=0,$$
        $$\mathcal L=\tfrac12(\hat y-y)^2=\tfrac12(0-2)^2=2.$$
        <p class="font-semibold mt-2">Backward:</p>
        $$\delta^{(2)}=\hat y-y= -2,$$
        $$\tfrac{\partial \mathcal L}{\partial w_2}=\delta^{(2)}\,h^{(1)}=
        -2\begin{bmatrix}3\\1\end{bmatrix}=\begin{bmatrix}-6\\-2\end{bmatrix},\quad
        \tfrac{\partial \mathcal L}{\partial b_2}= -2,$$
        $$\delta^{(1)}=(w_2\,\delta^{(2)})\odot
        f'(a^{(1)})=\begin{bmatrix}1\\-3\end{bmatrix}(-2)\odot\begin{bmatrix}1\\1\end{bmatrix}=\begin{bmatrix}-2\\6\end{bmatrix},$$
        $$\tfrac{\partial \mathcal L}{\partial
        W_1}=\delta^{(1)}x^\top=\begin{bmatrix}-2\\6\end{bmatrix}\begin{bmatrix}1 & -2\end{bmatrix}
        =\begin{bmatrix}-2 & 4\\ 6 & -12\end{bmatrix},\quad
        \tfrac{\partial \mathcal L}{\partial b_1}=\begin{bmatrix}-2\\6\end{bmatrix}.$$
      </div>
      <p class="mt-2 text-sm text-slate-700">(Since $a^{(1)}=[3,1]^\top>0$, $f'(a^{(1)})=\mathbf1$ for ReLU here.)</p>
    </article>

    <!-- Matrix calculus cheat -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
      <h3 class="text-xl font-semibold mb-2">General Matrix Calculus View</h3>
      <div class="math-block text-sm">
        For layer $a=W x + b$, $h=f(a)$, scalar loss $\mathcal L$:
        $$\frac{\partial \mathcal L}{\partial W} = \Big(\frac{\partial \mathcal L}{\partial
        a}\Big)\,x^\top,\qquad
        \frac{\partial \mathcal L}{\partial b} = \frac{\partial \mathcal L}{\partial a},\qquad
        \frac{\partial \mathcal L}{\partial x} = W^\top\,\frac{\partial \mathcal L}{\partial a}.$$
        With $h=f(a)$ elementwise, $\partial\mathcal L/\partial a = (\partial\mathcal L/\partial
        h)\odot f'(a)$.
      </div>
    </article>
  </section>

  <!-- ======= Initialization ======= -->
  <section id="initialization">
    <h2 class="text-3xl font-bold mb-2">
      <a class="heading-anchor" href="#initialization" aria-label="Anchor">#</a>
      Initialization
    </h2>

    <!-- Problem -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6">
      <h3 class="text-2xl font-bold">Vanishing/Exploding Gradient Problem</h3>
      <div class="mt-4 warn-box">
        Careless initialization can make activations and gradients shrink to zero or explode across
        layers, destabilizing training.
      </div>
      <div class="grid md:grid-cols-2 gap-4 mt-4 text-sm">
        <div class="bg-slate-50 p-4 rounded-lg border">
          <h4 class="font-semibold mb-2">Vanishing</h4>
          <ul class="list-disc pl-5 space-y-1 text-slate-700">
            <li>Small weights → shrinking activations</li>
            <li>Products of small Jacobians</li>
            <li>Early layers learn slowly</li>
          </ul>
        </div>
        <div class="bg-slate-50 p-4 rounded-lg border">
          <h4 class="font-semibold mb-2">Exploding</h4>
          <ul class="list-disc pl-5 space-y-1 text-slate-700">
            <li>Large weights → growing activations</li>
            <li>Unstable updates, oscillations</li>
            <li>Optimization breakdown</li>
          </ul>
        </div>
      </div>
    </article>

    <!-- Xavier -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6">
      <h3 class="text-2xl font-bold">Xavier (Glorot) Initialization</h3>
      <div class="mt-3 note-box">
        Goal: keep variance of signals comparable across layers in both forward and backward passes.
      </div>

      <div class="mt-4 math-block">
        <p class="mb-2">Linear layer: $y_i = \sum_{j=1}^{n_{in}} w_{ij} x_j$</p>
        <p class="mb-2">Assume $\mathbb{E}[x]=0$, $\operatorname{Var}(x)=\sigma_x^2$ and independent
          weights and inputs.</p>
        <div class="mt-3">
          $$\operatorname{Var}(y_i) = \sum_{j=1}^{n_{in}} \operatorname{Var}(w_{ij})
          \operatorname{Var}(x_j) = n_{in}\,\operatorname{Var}(w)\,\sigma_x^2.$$
          To preserve variance: $\operatorname{Var}(y) \approx \sigma_x^2 \Rightarrow
          \operatorname{Var}(w)=\tfrac{1}{n_{in}}$.
          Backward analysis yields $\operatorname{Var}(w)=\tfrac{1}{n_{out}}$. Use the average:
          $$\operatorname{Var}(w) = \frac{2}{n_{in}+n_{out}}.$$
        </div>
      </div>

      <div class="grid md:grid-cols-2 gap-4 mt-4 text-sm">
        <div class="bg-blue-50 border rounded-lg p-4">
          <h4 class="font-semibold text-blue-900">Normal</h4>
          $w \sim \mathcal{N}\!\Big(0,\, \sqrt{\tfrac{2}{n_{in}+n_{out}}}\Big)$
        </div>
        <div class="bg-blue-50 border rounded-lg p-4">
          <h4 class="font-semibold text-blue-900">Uniform</h4>
          $w \sim \mathcal{U}\!\Big(-\sqrt{\tfrac{6}{n_{in}+n_{out}}},\,
          +\sqrt{\tfrac{6}{n_{in}+n_{out}}}\Big)$
        </div>
      </div>

      <p class="text-sm text-slate-600 mt-3"><strong>Use with:</strong> tanh/sigmoid/linear layers, LSTM gates.</p>
    </article>

    <!-- He -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6">
      <h3 class="text-2xl font-bold">He Initialization</h3>
      <div class="mt-3 note-box">
        For ReLU-like activations, roughly half of the mass is zeroed; compensate by doubling the variance.
      </div>

      <div class="mt-4 math-block">
        <p class="mb-2">For $x\sim \mathcal{N}(0,\sigma^2)$ and $f(x)=\max(0,x)$:</p>
        $$\mathbb{E}[f(x)] = \frac{\sigma}{\sqrt{2\pi}},\qquad \operatorname{Var}[f(x)] = \frac{\sigma^2}{2}.$$
        <p class="mt-2 mb-2">Forward variance preservation leads to</p>
        $$\operatorname{Var}(w) = \frac{2}{n_{in}}.$$
      </div>

      <div class="grid md:grid-cols-2 gap-4 mt-4 text-sm">
        <div class="bg-green-50 border rounded-lg p-4">
          <h4 class="font-semibold text-green-900">He Normal</h4>
          $w \sim \mathcal{N}\!\Big(0,\, \sqrt{\tfrac{2}{n_{in}}}\Big)$
        </div>
        <div class="bg-green-50 border rounded-lg p-4">
          <h4 class="font-semibold text-green-900">He Uniform</h4>
          $w \sim \mathcal{U}\!\Big(-\sqrt{\tfrac{6}{n_{in}}},\, +\sqrt{\tfrac{6}{n_{in}}}\Big)$
        </div>
      </div>

      <p class="text-sm text-slate-600 mt-3"><strong>Use with:</strong> ReLU, Leaky ReLU, ELU, SiLU.</p>
    </article>

    <!-- Playground -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6"  x-data="initPlayground()">
      <h3 class="text-2xl font-bold">Initialization Playground</h3>
      <p class="text-sm text-slate-600 mt-1">Compute recommended std/range for your layer shape and activation.</p>

      <div class="grid md:grid-cols-4 gap-4 mt-4">
        <div>
          <label class="text-xs font-medium text-slate-600">Fan-in (n_in)</label>
          <input type="number" class="mt-1 w-full border rounded-lg px-3 py-2"
                 x-model.number="nin" min="1">
        </div>
        <div>
          <label class="text-xs font-medium text-slate-600">Fan-out (n_out)</label>
          <input type="number" class="mt-1 w-full border rounded-lg px-3 py-2"
                 x-model.number="nout" min="1">
        </div>
        <div>
          <label class="text-xs font-medium text-slate-600">Activation</label>
          <select class="mt-1 w-full border rounded-lg px-3 py-2" x-model="act">
            <option value="relu">ReLU / Leaky / ELU</option>
            <option value="tanh">tanh / sigmoid / linear</option>
          </select>
        </div>
        <div class="flex items-end">
          <button class="w-full bg-indigo-600 text-white font-semibold px-4 py-2 rounded-lg hover:bg-indigo-700"
                  @click="recalc()">Recalculate</button>
        </div>
      </div>

      <div class="grid md:grid-cols-2 gap-4 mt-5 text-sm">
        <div class="bg-slate-50 border rounded-lg p-4">
          <h4 class="font-semibold">Normal std</h4>
          <p class="mt-1 font-mono"><span x-text="normalStd.toFixed(6)"></span></p>
          <p class="text-xs text-slate-500 mt-2" x-show="act==='relu'">He: $\sigma=\sqrt{2/n_{in}}$</p>
          <p class="text-xs text-slate-500 mt-2" x-show="act==='tanh'">Xavier: $\sigma=\sqrt{2/(n_{in}+n_{out})}$</p>
        </div>
        <div class="bg-slate-50 border rounded-lg p-4">
          <h4 class="font-semibold">Uniform range</h4>
          <p class="mt-1 font-mono">[-<span x-text="uniformA.toFixed(6)"></span>, +<span x-text="uniformA.toFixed(6)"></span>]</p>
          <p class="text-xs text-slate-500 mt-2" x-show="act==='relu'">He: $a=\sqrt{6/n_{in}}$</p>
          <p class="text-xs text-slate-500 mt-2" x-show="act==='tanh'">Xavier: $a=\sqrt{6/(n_{in}+n_{out})}$</p>
        </div>
      </div>

      <details class="mt-5">
        <summary class="font-semibold cursor-pointer">PyTorch & Keras snippets</summary>
        <pre class="code bg-slate-900 text-slate-100 p-4 rounded-lg overflow-x-auto mt-3"><code># PyTorch
torch.nn.init.xavier_normal_(layer.weight)   # tanh/sigmoid
torch.nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')

# Keras
Dense(units, kernel_initializer='glorot_normal')
Dense(units, kernel_initializer='he_normal')</code></pre>
      </details>
    </article>
  </section>

  <!-- ======= Dropout ======= -->
  <section id="dropout">
    <h2 class="text-3xl font-bold mb-2">
      <a class="heading-anchor" href="#dropout" aria-label="Anchor">#</a>
      Dropout
    </h2>

    <!-- Definition & Interactive -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6"  x-data="dropoutDemo()">
      <h3 class="text-2xl font-bold">Definition & Interactive Visual</h3>

      <div class="mt-3 note-box">
        During training, randomly zero activations with probability \(p\) and scale by \(\frac{1}{1-p}\)
        so that the <strong>expectation matches inference</strong>.
      </div>

      <div class="mt-4 math-block">
        <p class="mb-2"><strong>Training (inverted dropout):</strong></p>
        $$r_i \sim \mathrm{Bernoulli}(1-p),\qquad \tilde h_i=\frac{r_i\,h_i}{1-p}.$$

        <ul class="text-sm mt-2 list-disc pl-6">
          <li>Keep w.p. \(1-p\): \(\tilde h_i = \dfrac{h_i}{1-p}\)</li>
          <li>Drop w.p. \(p\): \(\tilde h_i = 0\)</li>
        </ul>

        <p class="mt-3"><strong>Expectation & variance (for fixed \(h_i=h\)):</strong></p>
        $$\mathbb{E}[\tilde h]=h,\qquad
          \mathrm{Var}[\tilde h]
          = \frac{h^2}{1-p}-h^2
          = \frac{p}{1-p}\,h^2.$$

        <p class="mt-3"><strong>Inference:</strong> \(\tilde h=h\).</p>
      </div>

      <div class="grid md:grid-cols-2 gap-6 mt-5">
        <!-- Left demo -->
        <article>
          <h4 class="font-semibold mb-2">
            Training snapshot (p = <span x-text="p.toFixed(2)"></span>)
          </h4>
          <div class="flex flex-wrap gap-2">
            <template x-for="(node, i) in training" :key="i">
              <div class="neuron" :class="node ? 'active' : 'dropped'">
                <span x-text="i+1"></span>
              </div>
            </template>
          </div>

          <div class="mt-3 grid grid-cols-3 gap-2 items-end">
            <div>
              <label class="text-xs text-slate-600 block">Drop prob p</label>
              <input type="range" min="0" max="0.8" step="0.05"
                     x-model.number="p" @input="rnd()" class="w-full">
            </div>
            <div>
              <label class="text-xs text-slate-600 block">Unit value h</label>
              <input type="range" min="0.1" max="2.0" step="0.1"
                     x-model.number="h" @input="redrawVar()" class="w-full">
            </div>
            <div class="flex">
              <button class="bg-slate-800 text-white px-3 py-2 rounded-lg hover:bg-slate-700 w-full"
                      @click="rnd()">New step</button>
            </div>
          </div>
        </article>

        <!-- Right stats + plot -->
        <article>
          <h4 class="font-semibold mb-2">Inference snapshot (p = 0)</h4>
          <div class="flex flex-wrap gap-2">
            <template x-for="(node, i) in inference" :key="i">
              <div class="neuron active"><span x-text="i+1"></span></div>
            </template>
          </div>

          <div class="mt-3 grid grid-cols-3 gap-3 text-sm">
            <div class="bg-slate-50 border rounded-lg p-3">
              <div class="text-xs text-slate-600">Scaling</div>
              <div class="font-mono" x-text="(1/(1-p)).toFixed(3)"></div>
            </div>
            <div class="bg-slate-50 border rounded-lg p-3">
              <div class="text-xs text-slate-600">E[~h] (for h)</div>
              <div class="font-mono" x-text="h.toFixed(3)"></div>
            </div>
            <div class="bg-slate-50 border rounded-lg p-3">
              <div class="text-xs text-slate-600">Var[~h] (for h)</div>
              <div class="font-mono" x-text="( (p/(1-p))*h*h ).toFixed(3)"></div>
            </div>
          </div>

          <div class="mt-4 bg-white border rounded-lg p-3">
            <h5 class="font-medium text-sm mb-2">Variance vs dropout rate</h5>
            <canvas id="dropoutVarCanvas" width="520" height="160" class="w-full"></canvas>
            <p class="text-[11px] text-slate-500 mt-1">
              For fixed \(h\): \(\mathrm{Var}[\tilde h]=\dfrac{p}{1-p}h^2\). Curve shown for \(p\in[0,0.95]\);
              marker indicates current \(p\).
            </p>
          </div>
        </article>
      </div>
    </article>

    <!-- Theory -->
  <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6" x-data="dropoutTheory()">
  <h3 class="text-2xl font-bold">Theory</h3>

  <div class="theorem-box mt-2">
    Dropout training approximates an ensemble over \(2^n\) subnetworks with shared parameters.
  </div>

  <div class="mt-4 math-block">
    <p class="font-semibold mb-1">Step 1 — Subnetworks & active units</p>
    $$\#\text{subnetworks}=2^n,\qquad \mathbb{E}[\text{active units}]=n(1-p).$$

    <p class="font-semibold mt-3 mb-1">Step 2 — Ensemble view</p>
    $$f_{\text{drop}}(x)\approx \frac{1}{M}\sum_{m=1}^M f_m(x),\quad f_m:\text{ sampled subnetwork.}$$

    <p class="font-semibold mt-3 mb-1">Step 3 — Variance reduction with correlation \(\rho\)</p>
    $$\mathrm{Var}[\bar f]=\frac{\sigma^2}{M}\bigl(1+(M-1)\rho\bigr).$$
    <p class="text-xs text-slate-300">Low correlation between subnetworks (\(\rho\downarrow\)) improves averaging.</p>
  </div>

  <div class="mt-4 proof-box">
    <h4 class="font-semibold mb-1">Bias–Variance view</h4>
    <p class="text-sm">Averaging many subnetworks reduces variance roughly like \(1/M\) when \(\rho\) is small; bias changes are usually minor.</p>
  </div>

  <div class="mt-4 math-block">
    <h4 class="font-semibold mb-2">Step 4 — Gaussian approximation of Bernoulli noise</h4>
    $$r_i \sim \mathrm{Bern}(1-p)\ \approx\ \mathcal{N}(1-p,\;p(1-p))\quad\Longrightarrow\quad
      \mathcal{L}_{\text{dropout}}\approx \mathcal{L} + \lambda\,p(1-p)\sum_i \mathbb{E}[h_i^2].$$
    <p class="text-xs text-slate-300">Acts like <strong>data-dependent shrinkage</strong> (units with larger activations get more regularized).</p>
  </div>

  <!-- Tiny interactive -->
  <div class="mt-5 grid md:grid-cols-2 gap-4">
    <article class="bg-slate-50 border rounded-lg p-4">
      <h4 class="font-semibold">Ensemble variance (normalized by \(\sigma^2\))</h4>
      <label class="text-xs text-slate-600 block"># models \(M\)</label>
      <input type="range" min="1" max="256" step="1" x-model.number="M" @input="recalc()" class="w-full">

      <label class="text-xs text-slate-600 block mt-3">Correlation \(\rho\)</label>
      <input type="range" min="0" max="0.99" step="0.01" x-model.number="rho" @input="recalc()" class="w-full">

      <div class="mt-3 text-sm grid grid-cols-2 gap-2">
        <div class="bg-white border rounded p-2">
          <div class="text-xs text-slate-600">Var ratio</div>
          <div class="font-mono" x-text="ratio.toFixed(4)"></div>
        </div>
        <div class="bg-white border rounded p-2">
          <div class="text-xs text-slate-600">Effective models</div>
          <div class="font-mono" x-text="(1/ratio).toFixed(2)"></div>
        </div>
      </div>
    </article>

    <article class="bg-slate-50 border rounded-lg p-4">
      <h4 class="font-semibold">Curve vs \(M\) (fixed \(\rho\))</h4>
      <!-- Use x-ref instead of id -->
      <canvas x-ref="ensVarCanvas" width="520" height="160" class="w-full"></canvas>
      <p class="text-[11px] text-slate-500 mt-1">
        Plots \(\mathrm{Var}[\bar f]/\sigma^2 = \dfrac{1+(M-1)\rho}{M}\) for \(M=1..256\); marker at your selected \(M\).
      </p>
    </article>
  </div>
</article>


    <!-- Practice -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-6">
      <h3 class="text-2xl font-bold">Practical Guidelines</h3>

      <div class="grid md:grid-cols-2 gap-4 mt-3 text-sm">
        <article class="bg-slate-50 border rounded-lg p-4">
          <h4 class="font-semibold">Rules of thumb</h4>
          <ul class="list-disc pl-5 space-y-1">
            <li>Input layer: \(p \approx 0.1\)–\(0.2\)</li>
            <li>Hidden layers: \(p \approx 0.5\)</li>
            <li>CNNs: \(p \approx 0.1\)–\(0.3\)</li>
            <li>Output layer: \(p = 0\)</li>
          </ul>
        </article>
        <article class="bg-slate-50 border rounded-lg p-4">
          <h4 class="font-semibold">When to avoid</h4>
          <ul class="list-disc pl-5 space-y-1">
            <li>Alongside BatchNorm in the same block</li>
            <li>Recurrent connections (prefer variational/locked dropout: same mask across time)</li>
            <li>Very small models or when strong alternative regularization is present</li>
          </ul>
        </article>
      </div>

      <details class="mt-5">
        <summary class="font-semibold cursor-pointer">PyTorch example</summary>
<pre class="code bg-slate-900 text-slate-100 p-4 rounded-lg overflow-x-auto mt-3"><code>import torch, torch.nn as nn, torch.nn.functional as F

class DropoutNet(nn.Module):
    def __init__(self, d_in, d_hid, d_out):
        super().__init__()
        self.fc1 = nn.Linear(d_in, d_hid)
        self.do1 = nn.Dropout(p=0.2)   # input-ish
        self.fc2 = nn.Linear(d_hid, d_hid)
        self.do2 = nn.Dropout(p=0.5)   # hidden
        self.fc3 = nn.Linear(d_hid, d_out)
    def forward(self, x):
        x = F.relu(self.fc1(x)); x = self.do1(x)
        x = F.relu(self.fc2(x)); x = self.do2(x)
        return self.fc3(x)
</code></pre>
      </details>

      <details class="mt-3">
        <summary class="font-semibold cursor-pointer">Variational/locked dropout (RNN sketch)</summary>
<pre class="code bg-slate-900 text-slate-100 p-4 rounded-lg overflow-x-auto mt-3"><code># Same dropout mask over all time steps (improves temporal consistency)
class LockedDropout(nn.Module):
    def __init__(self, p):
        super().__init__(); self.p = p
    def forward(self, x):  # x: (T, B, H)
        if not self.training or self.p == 0: return x
        mask = x.new_empty(1, x.size(1), x.size(2)).bernoulli_(1 - self.p) / (1 - self.p)
        return x * mask  # broadcast across time
</code></pre>
      </details>
    </article>
  </section>

  <!-- ======= Normalization Layers ======= -->
  <section id="norms">
    <h2 class="text-3xl font-bold mb-2">
      <a class="heading-anchor" href="#norms" aria-label="Anchor">#</a>
      Normalization Layers
    </h2>

    <!-- Intro -->
    <article class="bg-white/60 border border-slate-200 rounded-2xl p-4 shadow-sm mb-2">
      <p class="text-slate-700">
        Mathematics for BatchNorm, LayerNorm, and RMSNorm: definitions,
        forward maps, gradient sketches, inference behavior, and quick comparison.
      </p>
    </article>

    <!-- BatchNorm -->
    <article id="batchnorm" class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-8">
      <h3 class="text-2xl font-semibold mb-2">Batch Normalization (BatchNorm)</h3>
      <div class="note-box text-sm">
        Normalize each feature using mini-batch statistics; then learnable affine $(\gamma,\beta)$ restores scale/shift.
        Helps stabilize depth and allows larger learning rates.
      </div>
      <div class="math-block text-sm mt-3">
        <p class="font-semibold">Forward (dense: batch m × features d; conv: stats over N×H×W):</p>
        $$\mu_B=\frac{1}{m}\sum_{i=1}^{m}x_i,\quad
        \sigma_B^2=\frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_B)^2$$
        $$\hat x_i=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\varepsilon}},\quad y_i=\gamma\,\hat x_i+\beta$$
        <p class="mt-2">Running stats for inference (momentum $\alpha$):</p>
        $$\mu_{\text{run}}\leftarrow(1-\alpha)\mu_{\text{run}}+\alpha\mu_B,\quad
        \sigma^2_{\text{run}}\leftarrow(1-\alpha)\sigma^2_{\text{run}}+\alpha\sigma_B^2$$
      </div>

      <details class="mt-3 bg-slate-50 border border-slate-200 rounded-xl p-4">
        <summary class="font-semibold text-slate-800 cursor-pointer">Gradient sketch</summary>
        <div class="math-block text-sm mt-3">
          Let $g_i=\partial\mathcal L/\partial y_i$. Then
          $$\frac{\partial\mathcal L}{\partial \beta}=\sum_i g_i,\qquad
          \frac{\partial\mathcal L}{\partial \gamma}=\sum_i g_i\,\hat x_i.$$
          Inputs:
          $$\frac{\partial\mathcal L}{\partial x_i}=
          \frac{\gamma}{\sqrt{\sigma_B^2+\varepsilon}}\Big(g_i-\overline g-\hat x_i\,\overline{g\hat x}\Big),$$
          where $\overline g=\tfrac{1}{m}\sum_j g_j$ and $\overline{g\hat x}=\tfrac{1}{m}\sum_j g_j\hat x_j$.
        </div>
      </details>

      <div class="grid md:grid-cols-2 gap-4 mt-4">
        <div class="bg-slate-50 border border-slate-200 rounded-xl p-4">
          <h4 class="font-medium">When BN shines</h4>
          <ul class="list-disc ml-5 text-sm text-slate-700 space-y-1">
            <li>Large batches; CNNs (channelwise stats over spatial dims)</li>
            <li>Acts as regularizer; supports higher LR</li>
          </ul>
        </div>
        <div class="bg-slate-50 border border-slate-200 rounded-xl p-4">
          <h4 class="font-medium">Caveats</h4>
          <ul class="list-disc ml-5 text-sm text-slate-700 space-y-1">
            <li>Small batches → noisy/biased stats</li>
            <li>Autoregressive/sequence models: batch coupling is undesirable</li>
          </ul>
        </div>
      </div>

      <details class="mt-4 bg-slate-50 border border-slate-200 rounded-xl p-4">
        <summary class="font-semibold text-slate-800 cursor-pointer">PyTorch reference</summary>
        <pre class="code text-sm bg-slate-900 text-slate-100 rounded-lg p-3 overflow-x-auto">nn.BatchNorm1d(d, eps=1e-5, momentum=0.1)
# Conv: nn.BatchNorm2d(C), stats over (N,H,W)
</pre>
      </details>
    </article>

    <!-- LayerNorm -->
    <article id="layernorm" class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-8">
      <h3 class="text-2xl font-semibold mb-2">Layer Normalization (LayerNorm)</h3>
      <div class="note-box text-sm">
        Normalize across the <em>features of each example</em>. No batch dependency → perfect for RNNs and transformers.
        Train/test are identical.
      </div>
      <div class="math-block text-sm mt-3">
        <p class="font-semibold">Forward (per sample with H features):</p>
        $$\mu=\frac{1}{H}\sum_{j=1}^{H}x_j,\quad \sigma^2=\frac{1}{H}\sum_{j=1}^{H}(x_j-\mu)^2$$
        $$\hat x_j=\frac{x_j-\mu}{\sqrt{\sigma^2+\varepsilon}},\quad y_j=\gamma_j\,\hat x_j+\beta_j$$
      </div>

      <details class="mt-3 bg-slate-50 border border-slate-200 rounded-xl p-4">
        <summary class="font-semibold text-slate-800 cursor-pointer">Notes & gradients</summary>
        <div class="math-block text-sm mt-3">
          Same algebra as BN but averages over features $H$ (not batch $m$). Invariant to
          feature-wise affine transforms shared within a sample.
        </div>
      </details>

      <details class="mt-4 bg-slate-50 border border-slate-200 rounded-xl p-4">
        <summary class="font-semibold text-slate-800 cursor-pointer">PyTorch reference</summary>
        <pre class="code text-sm bg-slate-900 text-slate-100 rounded-lg p-3 overflow-x-auto">nn.LayerNorm(H, eps=1e-5)</pre>
      </details>
    </article>

    <!-- RMSNorm -->
    <article id="rmsnorm" class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mb-2">
      <h3 class="text-2xl font-semibold mb-2">RMSNorm</h3>
      <div class="note-box text-sm">
        Like LayerNorm but <em>without mean-centering</em>. Normalizes only by root-mean-square;
        typically scale-only ($\gamma$) parameter. Often used in large LMs.
      </div>
      <div class="math-block text-sm mt-3">
        <p class="font-semibold">Forward (per sample, H features):</p>
        $$\operatorname{rms}(x)=\sqrt{\frac{1}{H}\sum_{j=1}^{H}x_j^2+\varepsilon},\qquad
        y_j=\gamma_j\,\frac{x_j}{\operatorname{rms}(x)}$$
      </div>

      <details class="mt-3 bg-slate-50 border border-slate-200 rounded-xl p-4">
        <summary class="font-semibold text-slate-800 cursor-pointer">Remarks</summary>
        <div class="math-block text-sm mt-3">
          Preserves direction of $x$ and rescales its norm; avoids the feature-mean dependency,
          which can improve stability with certain attention/activation mixes.
        </div>
      </details>

      <details class="mt-4 bg-slate-50 border border-slate-200 rounded-xl p-4">
        <summary class="font-semibold text-slate-800 cursor-pointer">PyTorch-like reference</summary>
        <pre class="code text-sm bg-slate-900 text-slate-100 rounded-lg p-3 overflow-x-auto">class RMSNorm(nn.Module):
    def __init__(self, H, eps=1e-6):
        super().__init__(); self.eps=eps; self.scale=nn.Parameter(torch.ones(H))
    def forward(self, x):  # x: (..., H)
        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()
        return x / rms * self.scale</pre>
      </details>
    </article>

    <!-- Quick comparison -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card mt-8">
      <h3 class="text-xl font-semibold mb-3">Quick Comparison</h3>
      <div class="overflow-x-auto">
        <table class="min-w-full text-sm">
          <thead class="bg-slate-50 text-slate-700">
            <tr>
              <th class="text-left p-2">Norm</th>
              <th class="text-left p-2">Axes normalized</th>
              <th class="text-left p-2">Train/Test mismatch?</th>
              <th class="text-left p-2">Params</th>
              <th class="text-left p-2">Typical use</th>
            </tr>
          </thead>
          <tbody>
            <tr class="border-t">
              <td class="p-2 font-medium">BatchNorm</td>
              <td class="p-2">per feature across batch (and spatial)</td>
              <td class="p-2">Yes (running stats)</td>
              <td class="p-2">$\gamma,\beta$</td>
              <td class="p-2">CNNs, large batches</td>
            </tr>
            <tr class="border-t">
              <td class="p-2 font-medium">LayerNorm</td>
              <td class="p-2">per sample across features</td>
              <td class="p-2">No</td>
              <td class="p-2">$\gamma,\beta$ (per feature)</td>
              <td class="p-2">Transformers, RNNs</td>
            </tr>
            <tr class="border-t">
              <td class="p-2 font-medium">RMSNorm</td>
              <td class="p-2">per sample, RMS only</td>
              <td class="p-2">No</td>
              <td class="p-2">$\gamma$ (scale only)</td>
              <td class="p-2">Large LMs (pre/post-attention)</td>
            </tr>
          </tbody>
        </table>
      </div>
    </article>
  </section>

      <h2 class="text-3xl font-bold mb-2">
      <a class="heading-anchor" href="#momentum-adam" aria-label="Anchor">#</a>
      Momentum / Adam — Fixed-Point Intuition
    </h2>
  <!-- ======= Momentum / Adam ======= -->
  <section class="card p-6" id="momentum-adam" x-data="adamFixed()">


    <article class="space-y-3">
      <div class="math-block">
        <p class="font-semibold mb-1">Step 1 — Momentum as exponential smoothing</p>
        $$v_t=\beta v_{t-1}+g_t \;\;\Longrightarrow\;\; v_t=(1-\beta)\sum_{k=0}^{t-1}\beta^k g_{t-k}.$$
        Momentum averages recent gradients with geometric weights; update: \(w_{t+1}=w_t-\eta v_t\).
      </div>

      <div class="math-block">
        <p class="font-semibold mb-1">Step 2 — Adam moments & bias corrections</p>
        $$m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t,\quad v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2,$$
        $$\hat m_t=\frac{m_t}{1-\beta_1^t},\qquad \hat v_t=\frac{v_t}{1-\beta_2^t},\qquad
        \Delta w_t=-\eta\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}.$$
        <span class="text-xs text-slate-300">Bias correction removes the startup shrink because
          \(m_t,v_t\) start at 0.</span>
      </div>

      <div class="note-box">
        <strong>Intuition.</strong> Adam scales the step by an estimate of recent gradient RMS
        (\(\sqrt{\hat v_t}\)): flat/low-variance directions → larger steps; spiky/high-variance → smaller steps.
      </div>
    </article>

    <article class="mt-5 grid md:grid-cols-2 gap-4">
      <section class="bg-slate-50 border rounded-lg p-4">
        <h4 class="font-semibold">Gradient pattern</h4>
        <select class="w-full border rounded px-2 py-1 text-sm" x-model="pattern" @change="recompute()">
          <option value="const">constant (|g|)</option>
          <option value="alt">alternating sign</option>
          <option value="noisy">noisy (Gaussian)</option>
        </select>

        <label class="text-xs text-slate-600 block mt-3">|g| (magnitude)</label>
        <input type="range" min="0.01" max="1.0" step="0.01" x-model.number="gMag" @input="recompute()" class="w-full">

        <label class="text-xs text-slate-600 block mt-3">β<sub>1</sub> (momentum)</label>
        <input type="range" min="0.0" max="0.999" step="0.001" x-model.number="b1" @input="recompute()" class="w-full">

        <label class="text-xs text-slate-600 block mt-3">β<sub>2</sub> (RMS)</label>
        <input type="range" min="0.0" max="0.999" step="0.001" x-model.number="b2" @input="recompute()" class="w-full">

        <label class="text-xs text-slate-600 block mt-3">Steps T</label>
        <input type="range" min="10" max="400" step="1" x-model.number="T" @input="recompute()" class="w-full">

        <p class="text-xs mt-2">
          Bias factors at \(t=T\): \(1-\beta_1^T\)=<span class="font-mono" x-text="(1-Math.pow(b1,T)).toFixed(4)"></span>,
          \(1-\beta_2^T\)=<span class="font-mono" x-text="(1-Math.pow(b2,T)).toFixed(4)"></span>
        </p>
      </section>

      <article class="bg-slate-50 border rounded-lg p-4">
        <h4 class="font-semibold">Effective step (Adam-style, η=1)</h4>
        <div class="mt-2 text-sm grid grid-cols-3 gap-2">
          <div>m̂(T): <span class="font-mono" x-text="last.mhat.toFixed(6)"></span></div>
          <div>√v̂(T): <span class="font-mono" x-text="Math.sqrt(last.vhat).toFixed(6)"></span></div>
          <div>|Δw(T)|: <span class="font-mono" x-text="last.step.toFixed(6)"></span></div>
        </div>
        <p class="text-xs text-slate-600 mt-2">
          Larger β₂ ↑ → slower RMS adaptation (smoother denominator). Larger β₁ ↑ → smoother numerator (momentum).
        </p>
      </article>
    </article>

    <article class="mt-5 bg-white border rounded-lg p-4">
      <h4 class="font-semibold mb-2">Evolution over steps (t = 1…T)</h4>
      <p class="text-xs text-slate-600 mb-2">We plot \(g_t\) (gray), \(\hat m_t\) (blue), \(\sqrt{\hat v_t}\) (orange), and \(|\Delta w_t|\) (green).</p>
      <div class="relative">
        <canvas id="adamPlot" width="700" height="180" class="w-full"></canvas>
      </div>
      <div class="text-[11px] text-slate-500 mt-2">
        Scales are auto-normalized to [0,1] per series to keep all curves visible; compare relative shapes, not absolute magnitudes.
      </div>
    </article>
  </section>

  <!-- ======= Spectral Norm / Lipschitz ======= -->
       <h2 class="text-3xl font-bold mb-2">
      <a class="heading-anchor" href="#lipschitz" aria-label="Anchor">#</a>
      Spectral Norm, Lipschitz & Gradient Flow
    </h2>
  <section class="card p-6" id="lipschitz" x-data="lipschitzDemo()">


    <article class="space-y-3">
      <div class="math-block">
        <p class="font-semibold mb-1">Step 1 — Operator norm = largest singular value</p>
        For a linear map \(x\mapsto Wx\),
        $$\|W\|_2 \;\equiv\; \sup_{\|x\|_2=1}\|Wx\|_2 \;=\; \sigma_{\max}(W) \;=\; \sqrt{\lambda_{\max}(W^\top W)}.$$
      </div>

      <div class="math-block">
        <p class="font-semibold mb-1">Step 2 — Submultiplicativity</p>
        For any matrices \(A,B\): \(\|AB\|_2 \le \|A\|_2\,\|B\|_2\).
        For a depth-\(L\) network \(f(x)=W_L\phi_{L-1}\cdots W_1 x\) with activation Lipschitz constants
        \(L_{\phi_\ell}\) (e.g., ReLU/tanh have \(L_{\phi}\!\le\!1\)):
        $$\mathrm{Lip}(f)\;\le\;\prod_{\ell=1}^L \|W_\ell\|_2 \;\cdot\; \prod_{\ell=1}^{L-1} L_{\phi_\ell}.$$
      </div>

      <div class="math-block">
        <p class="font-semibold mb-1">Step 3 — Backprop Jacobian bound</p>
        The layer-wise Jacobian for pre-activations \(a_\ell=W_\ell h_{\ell-1}\) is
        $$J_f(x) \;=\; \prod_{\ell=1}^{L} D\phi_{\ell-1}(a_{\ell-1})\,W_\ell,$$
        with \(D\phi_{\ell-1}=\mathrm{diag}(\phi'_{\ell-1}(a_{\ell-1}))\).
        If \(\|D\phi_{\ell-1}\|_2 \le L_{\phi_\ell}\), then
        $$\|J_f(x)\|_2 \;\le\; \prod_{\ell=1}^{L} \|W_\ell\|_2 \;\cdot\; \prod_{\ell=1}^{L-1} L_{\phi_\ell}.$$
        Hence the same product controls forward signal scale and gradient flow (explosion/vanishing).
      </div>

      <div class="note-box">
        <strong>Dynamical isometry (intuition).</strong> Keeping singular values of each block near 1
        (orthogonal/spectral norm ≈ 1, residual scaling) makes \(\prod \approx 1\), stabilizing deep nets.
      </div>
    </article>

    <article class="mt-5 grid md:grid-cols-2 gap-4">
      <section class="bg-slate-50 border rounded-lg p-4">
        <h4 class="font-semibold">Per-layer bounds</h4>

        <div class="flex items-center gap-2 text-xs mt-1">
          <span class="px-2 py-1 rounded border cursor-pointer" @click="randomize()">randomize near 1</span>
          <span class="px-2 py-1 rounded border cursor-pointer" @click="add()">+ add layer</span>
          <span class="px-2 py-1 rounded border cursor-pointer" @click="remove()" x-show="L>1">− remove</span>
        </div>

        <template x-for="(layer, i) in layers" :key="i">
          <div class="mt-3 border-t pt-3">
            <div class="text-xs font-semibold mb-1">Layer <span x-text="i+1"></span></div>
            <label class="text-xs text-slate-600 block">Spectral norm
              σ<sub>ℓ</sub>=\(\|W_\ell\|_2\)</label>
            <input type="range" min="0.2" max="3.0" step="0.01" x-model.number="layer.sigma"
                   @input="update()" class="w-full">
            <div class="text-[11px] mt-1 font-mono">σ = <span x-text="layer.sigma.toFixed(3)"></span></div>

            <label class="text-xs text-slate-600 block mt-2">Activation Lip \(L_{\phi_\ell}\)
              (ReLU/tanh ≤ 1)</label>
            <input type="range" min="0.5" max="1.5" step="0.01" x-model.number="layer.Lphi"
                   @input="update()" class="w-full">
            <div class="text-[11px] mt-1 font-mono">L<sub>φ</sub> = <span x-text="layer.Lphi.toFixed(2)"></span></div>
          </div>
        </template>
      </section>

      <article class="bg-slate-50 border rounded-lg p-4">
        <h4 class="font-semibold">Products & Diagnostics</h4>

        <div class="text-sm mt-1">
          <div>Forward bound \(\prod\sigma_\ell \cdot \prod L_{\phi_\ell}\):
            <span class="font-mono" x-text="forward.toFixed(6)"></span>
          </div>
          <div>Backprop bound (same expression):
            <span class="font-mono" x-text="forward.toFixed(6)"></span>
          </div>
        </div>

        <div class="mt-3 text-sm">
          <div>log10(bound):
            <span class="font-mono" x-text="log10f.toFixed(3)"></span>
          </div>
          <p class="text-xs mt-1">
            Heuristic zones:
            <span class="px-1 rounded" :class="zoneClass">
              <span x-text="zoneText"></span>
            </span>
            <br>
            (<em>near 0</em> ~ stable, <em>≪0</em> vanishing, <em>≫0</em> exploding)
          </p>
        </div>

        <div class="mt-3 text-xs text-slate-600">
          Tip: aim for σ<sub>ℓ</sub>≈1 via orthogonal init or spectral normalization; if needed,
          add residual scaling \(x+\alpha F(x)\) with \(\alpha<1\).
        </div>

        <details class="mt-3">
          <summary class="font-semibold cursor-pointer text-sm">Estimate σ via power iteration</summary>
          <pre class="code bg-slate-900 text-slate-100 p-3 rounded-lg overflow-x-auto text-[12px]">
# Given weight matrix W (PyTorch)
u = torch.randn(W.size(1)); u = u / u.norm()
for _ in range(T):              # e.g., T=10
    v = (W @ u);  v = v / v.norm()
    u = (W.T @ v); u = u / u.norm()
sigma = torch.dot(v, W @ u)     # ≈ spectral norm ||W||_2
          </pre>
        </details>
      </article>
    </article>
  </section>

  <!-- ======= Residual Connections as ODE ======= -->
  <section class="card p-6" id="residual-ode" x-data="residualODE()">
    <h2 class="text-3xl font-bold mb-2">
      <a class="heading-anchor" href="#residual-ode" aria-label="Anchor">#</a>
      Residual Connections as ODE Discretization
    </h2>

    <article class="space-y-3">
      <div class="math-block">
        <p class="font-semibold mb-1">Step 1 — Continuous-depth view</p>
        Consider depth as a continuous variable \(s\). A very deep net can be idealized by
        $$\frac{dx}{ds} = F(x(s), s),\qquad x(0)=x_{\text{in}}.$$
      </div>

      <div class="math-block">
        <p class="font-semibold mb-1">Step 2 — Forward Euler discretization</p>
        Discretize with step size \(h>0\):
        $$x_{k+1} \approx x_k + h\,F(x_k, s_k) \quad\Longleftrightarrow\quad \textbf{ResBlock: } x_{k+1}=x_k+F(x_k).$$
        In practice, the residual branch (conv/MLP/attention) plays the role of \(h\,F\).
      </div>

      <div class="math-block">
        <p class="font-semibold mb-1">Step 3 — Local linearization</p>
        Around a point, linearize \(F(x)\approx Jx\) where \(J=\frac{\partial F}{\partial x}\) is the Jacobian.
        Then a plain (non-residual) stack behaves like \(x_{k+1}=Jx_k\) (per-layer factor \(\lambda\)),
        while a residual block behaves like
        $$x_{k+1}=(I+hJ)\,x_k \quad\Rightarrow\quad \text{per-layer factor } (1+h\lambda).$$
      </div>

      <div class="math-block">
        <p class="font-semibold mb-1">Step 4 — Stability conditions (per eigen-direction)</p>
        <ul class="list-disc pl-6 text-sm">
          <li><em>Plain</em> (no skip): stable if \(|\lambda|<1\) (multiplying by \(J\) each layer).</li>
          <li><em>Residual/Euler</em>: stable if \(|1+h\lambda|<1\). In the \(z=h\lambda\) plane,
            this is the <strong>unit disk centered at \(-1\)</strong>: \(|z+1|<1\).</li>
        </ul>
      </div>

      <div class="math-block">
        <p class="font-semibold mb-1">Step 5 — Depth-\(K\) amplification</p>
        Over \(K\) layers, per eigen-direction:
        $$\text{Plain: } \ |\lambda|^K,\qquad \text{Residual: } \ |1+h\lambda|^K.$$
      </div>

      <div class="note-box">
        <strong>Practical tie-ins.</strong> (i) Pre-activation ResNets initialize the last BN scale
        (γ) near 0 ⇒ effectively small \(h\) at start. (ii) Residual scaling (e.g., \(x+\alpha F(x)\)) tunes \(h\).
        (iii) Spectral normalization/orthogonal init keep \(\|J\|\) moderate so that \(1+h\lambda\) stays well-conditioned.
      </div>
    </article>

    <!-- Interactive: eigenvalues -->
    <article class="mt-6 grid md:grid-cols-2 gap-4">
      <section class="bg-slate-50 border rounded-lg p-4">
        <h4 class="font-semibold">Toy linearized block (per eigen-direction)</h4>

        <label class="text-xs text-slate-600 block">Real part \(a=\Re(\lambda)\)</label>
        <input type="range" min="-2.0" max="2.0" step="0.01" x-model.number="lamR" @input="run()" class="w-full">

        <label class="text-xs text-slate-600 block mt-3">Imag part \(b=\Im(\lambda)\)</label>
        <input type="range" min="-2.0" max="2.0" step="0.01" x-model.number="lamI" @input="run()" class="w-full">

        <label class="text-xs text-slate-600 block mt-3">Step \(h\)</label>
        <input type="range" min="0.01" max="1.5" step="0.01" x-model.number="h" @input="run()" class="w-full">

        <label class="text-xs text-slate-600 block mt-3">Depth \(K\)</label>
        <input type="range" min="1" max="200" step="1" x-model.number="K" @input="run()" class="w-full">

        <p class="text-xs mt-2">
          \(\lambda=a+ib=\) <span class="font-mono" x-text="lamR.toFixed(2)"></span> +
          <span class="font-mono" x-text="lamI.toFixed(2)"></span>i,
          &nbsp; \(h=\) <span class="font-mono" x-text="h.toFixed(2)"></span>
        </p>
      </section>

      <article class="bg-slate-50 border rounded-lg p-4">
        <h4 class="font-semibold">Plain vs Residual — per layer & over depth</h4>

        <div class="text-sm mt-1">
          <div>\(|\lambda|\) (plain per-layer): <span class="font-mono" x-text="modLam.toFixed(4)"></span></div>
          <div>\(|1+h\lambda|\) (residual per-layer): <span class="font-mono" x-text="modResid.toFixed(4)"></span></div>
        </div>

        <div class="mt-2 text-sm">
          <div>Plain amplification \(|\lambda|^K\): <span class="font-mono" x-text="plainK.toExponential(3)"></span></div>
          <div>Residual amplification \(|1+h\lambda|^K\): <span class="font-mono" x-text="residK.toExponential(3)"></span></div>
        </div>

        <div class="mt-3">
          <span class="inline-block px-2 py-1 rounded text-xs"
                :class="stable ? 'bg-emerald-100 text-emerald-700' : 'bg-rose-100 text-rose-700'">
            Euler stability (|1+hλ| &lt; 1):
            <strong x-text="stable ? 'INSIDE disk → stable' : 'OUTSIDE disk → unstable'"></strong>
          </span>
          <p class="text-xs text-slate-600 mt-2">
            Euler stability disk in the \(z=h\lambda\) plane is the unit disk centered at \(-1\): \(|z+1|<1\).
          </p>
        </div>

        <div class="mt-3 text-xs text-slate-600">
          <template x-if="lamI === 0 && lamR < 0">
            <p>For real negative \(\lambda\), the optimal step (minimizes \(|1+h\lambda|\)) is
              \(h^\*=-1/\lambda\). Here: \(h^\*=\)
              <span class="font-mono" x-text="(-1/lamR).toFixed(3)"></span>.
            </p>
          </template>
        </div>
      </article>
    </article>
  </section>
</main>

    </div>

    <!-- Footer -->
    <footer class="text-center text-slate-100/80 text-xs mt-10">
      <p>Designed for classroom teaching — clean math, clear intuition, and interactive demos. Add proofs and sections over time.</p>
    </footer>
  </div>

  <!-- TOC logic (H2) -->
  <script>
    function toc() {
      return {
        items: [],
        activeId: null,
        init() {
          const main = document.getElementById('main');
          if (!main) return;

          const hs = [...main.querySelectorAll('h2')];

          const used = new Set();
          const slugify = (str) => {
            let s = String(str).toLowerCase()
              .replace(/[\s\.\,\/\?\!\:\;\(\)\[\]\{\}'"`~@#\$%\^&\*\+=\\\|<>]+/g, '-')
              .replace(/^-+|-+$/g, '')
              .slice(0, 80);
            let base = s || 'section';
            let k = 1, cand = base;
            while (used.has(cand)) cand = `${base}-${k++}`;
            used.add(cand);
            return cand;
          };

          // Ensure ids + anchor marks
          this.items = hs.map(h => {
            if (!h.id) h.id = slugify(h.textContent.trim());
            if (!h.querySelector('.heading-anchor')) {
              const a = document.createElement('a');
              a.className = 'heading-anchor';
              a.href = '#' + h.id;
              a.textContent = '#';
              a.setAttribute('aria-label', 'Anchor');
              h.prepend(a);
            }
            return { id: h.id, title: h.textContent.trim() };
          });

          const io = new IntersectionObserver((entries) => {
            entries.forEach(e => { if (e.isIntersecting) this.activeId = e.target.id; });
          }, { rootMargin: '0px 0px -60% 0px', threshold: 0.1 });

          this.items.forEach(it => {
            const el = document.getElementById(it.id);
            if (el) io.observe(el);
          });

          if (location.hash) this.activeId = location.hash.slice(1);
        },
        scrollTo(id) {
          const el = document.getElementById(id);
          if (!el) return;
          el.scrollIntoView({ behavior: 'smooth', block: 'start' });
          history.replaceState(null, '', `#${id}`);
          this.activeId = id;
        }
      }
    }
  </script>

  <!-- Interactive logic -->
  <script>
    function initPlayground() {
      return {
        nin: 256,
        nout: 256,
        act: 'relu', // 'relu' or 'tanh'
        normalStd: 0,
        uniformA: 0,
        recalc() {
          if (this.act === 'relu') {
            // He
            this.normalStd = Math.sqrt(2 / this.nin);
            this.uniformA = Math.sqrt(6 / this.nin);
          } else {
            // Xavier
            const denom = this.nin + this.nout;
            this.normalStd = Math.sqrt(2 / denom);
            this.uniformA = Math.sqrt(6 / denom);
          }
        },
        init() { this.recalc(); }
      }
    }
  </script>

  <script>
    function dropoutDemo(){
      return {
        p: 0.5,
        h: 1.0,
        training: Array.from({ length: 12 }, () => true),
        inference: Array.from({ length: 12 }, () => true),

        rnd(){
          const keep = 1 - this.p;
          this.training = this.training.map(() => Math.random() < keep);
          this.redrawVar();
        },

        redrawVar(){
          const c = document.getElementById('dropoutVarCanvas');
          if (!c) return;
          const ctx = c.getContext('2d');
          const W = c.width, H = c.height;
          ctx.clearRect(0,0,W,H);

          // axes
          ctx.strokeStyle = '#e5e7eb'; ctx.lineWidth = 1;
          ctx.beginPath();
          ctx.moveTo(30, H-20); ctx.lineTo(W-10, H-20); // x
          ctx.moveTo(30, 10);   ctx.lineTo(30, H-20);   // y
          ctx.stroke();

          // curve: Var = (p/(1-p)) * h^2, p in [0, 0.95]
          const maxP = 0.95;
          const samples = 200;
          const xs = []; const ys = [];
          let maxVar = 0;
          for (let i=0;i<=samples;i++){
            const p = maxP * i / samples;
            const v = (p/(1-p)) * this.h * this.h;
            xs.push(p); ys.push(v);
            if (v > maxVar) maxVar = v;
          }
          maxVar = Math.max(maxVar, 1e-6);

          const X = p => 30 + (W-40) * (p / maxP);
          const Y = v => 10 + (H-30) * (1 - v / maxVar);

          // draw curve
          ctx.strokeStyle = '#2563eb'; ctx.lineWidth = 2;
          ctx.beginPath();
          for (let i=0;i<xs.length;i++){
            const x = X(xs[i]), y = Y(ys[i]);
            if (i===0) ctx.moveTo(x,y); else ctx.lineTo(x,y);
          }
          ctx.stroke();

          // marker at current p
          const vNow = (this.p/(1-this.p)) * this.h * this.h;
          const mx = X(Math.min(this.p, maxP-1e-6)), my = Y(vNow);
          ctx.fillStyle = '#2563eb';
          ctx.beginPath(); ctx.arc(mx, my, 3, 0, 2*Math.PI); ctx.fill();

          // labels
          ctx.fillStyle = '#374151'; ctx.font = '10px Inter, sans-serif';
          ctx.fillText('p', W-18, H-24);
          ctx.fillText('Var', 8, 20);
        },

        init(){ this.rnd(); }
      }
    }

  </script>
<script>
  // Make sure this is defined before Alpine initializes, or attach to window:
  // window.dropoutTheory = function() { ... }
  function dropoutTheory(){
    return {
      M: 64, rho: 0.2, ratio: 0.0,
      recalc(){
        this.ratio = (1 + (this.M - 1) * this.rho) / this.M;
        this.$nextTick(() => this.drawCurve()); // ensure canvas is in DOM
      },
      drawCurve(){
        const c = this.$refs.ensVarCanvas; // scoped to this article
        if (!c) return;
        const ctx = c.getContext('2d');
        const W = c.width, H = c.height;
        ctx.clearRect(0,0,W,H);

        // axes
        ctx.strokeStyle = '#e5e7eb'; ctx.lineWidth = 1;
        ctx.beginPath();
        ctx.moveTo(30, H-20); ctx.lineTo(W-10, H-20);
        ctx.moveTo(30, 10);   ctx.lineTo(30, H-20);
        ctx.stroke();

        // curve for M=1..256
        const Ms = 256;
        const vals = Array.from({length: Ms}, (_,k) => (1 + k * this.rho) / (k+1));
        const X = m => 30 + (W-40) * (m / (Ms-1));
        const Y = v => 10 + (H-30) * (1 - v); // max is 1

        ctx.strokeStyle = '#10b981'; ctx.lineWidth = 2;
        ctx.beginPath();
        for (let k=0;k<Ms;k++){
          const x = X(k), y = Y(vals[k]);
          if (k===0) ctx.moveTo(x,y); else ctx.lineTo(x,y);
        }
        ctx.stroke();

        // marker
        const vNow = (1 + (this.M-1)*this.rho) / this.M;
        const mx = X(this.M-1), my = Y(vNow);
        ctx.fillStyle = '#10b981';
        ctx.beginPath(); ctx.arc(mx, my, 3, 0, 2*Math.PI); ctx.fill();

        // labels
        ctx.fillStyle = '#374151'; ctx.font = '10px Inter, sans-serif';
        ctx.fillText('M', W-18, H-24);
        ctx.fillText('Var ratio', 4, 20);
      },
      init(){
        // if this function might not exist yet when Alpine parses, attach it to window
        this.recalc();
      }
    }
  }
</script>

  <script>
    function adamFixed() {
      return {
        pattern: 'const', // 'const' | 'alt' | 'noisy'
        gMag: 0.2, b1: 0.9, b2: 0.999, T: 120, eps: 1e-8,
        series: { g: [], mhat: [], vhat: [], step: [] },
        last: { mhat: 0, vhat: 0, step: 0 },

        makeGradients() {
          const g = [];
          if (this.pattern === 'const') {
            for (let t = 1; t <= this.T; t++) g.push(this.gMag);
          } else if (this.pattern === 'alt') {
            for (let t = 1; t <= this.T; t++) g.push((t % 2 === 0 ? 1 : -1) * this.gMag);
          } else { // noisy
            for (let t = 1; t <= this.T; t++) {
              const u1 = Math.random(), u2 = Math.random();
              const z = Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
              g.push(z * this.gMag);
            }
          }
          return g;
        },

        compute() {
          const g = this.makeGradients();
          const mhat = new Array(this.T).fill(0);
          const vhat = new Array(this.T).fill(0);
          const step = new Array(this.T).fill(0);

          let mt = 0, vt = 0;
          for (let t = 1; t <= this.T; t++) {
            const gt = g[t - 1];
            mt = this.b1 * mt + (1 - this.b1) * gt;
            vt = this.b2 * vt + (1 - this.b2) * (gt * gt);
            const bc1 = 1 - Math.pow(this.b1, t);
            const bc2 = 1 - Math.pow(this.b2, t);
            const mht = (bc1 > 0 ? mt / bc1 : 0);
            const vht = (bc2 > 0 ? vt / bc2 : 0);
            mhat[t - 1] = mht;
            vhat[t - 1] = vht;
            step[t - 1] = Math.abs(mht) / (Math.sqrt(vht) + this.eps); // η=1
          }

          this.series = { g, mhat, vhat, step };
          this.last = { mhat: mhat[this.T - 1], vhat: vhat[this.T - 1], step: step[this.T - 1] };
        },

        draw() {
          const c = document.getElementById('adamPlot');
          if (!c) return;
          const ctx = c.getContext('2d');
          const W = c.width, H = c.height;
          ctx.clearRect(0, 0, W, H);

          // axes
          ctx.strokeStyle = '#e5e7eb';
          ctx.lineWidth = 1;
          ctx.beginPath();
          ctx.moveTo(40, H - 24); ctx.lineTo(W - 10, H - 24); // x-axis
          ctx.moveTo(40, 10); ctx.lineTo(40, H - 24);     // y-axis
          ctx.stroke();

          // helper to normalize series to [0,1]
          const norm = arr => {
            const mn = Math.min(...arr), mx = Math.max(...arr);
            if (!isFinite(mn) || !isFinite(mx) || mx - mn < 1e-12) return arr.map(_ => 0.5);
            return arr.map(v => (v - mn) / (mx - mn));
          };

          const n = this.T;
          const X = t => 40 + (W - 50) * (t / (n - 1));
          const Y = y => 10 + (H - 34) * (1 - y);

          const gN = norm(this.series.g);
          const mN = norm(this.series.mhat);
          const svN = norm(this.series.vhat.map(v => Math.sqrt(Math.max(0, v))));
          const stN = norm(this.series.step);

          const drawLine = (arr, color) => {
            ctx.strokeStyle = color; ctx.lineWidth = 1.5; ctx.beginPath();
            for (let t = 0; t < n; t++) {
              const x = X(t), y = Y(arr[t]);
              if (t === 0) ctx.moveTo(x, y); else ctx.lineTo(x, y);
            }
            ctx.stroke();
          };

          // series
          drawLine(gN, '#9ca3af'); // gray: g_t
          drawLine(mN, '#2563eb'); // blue: mhat
          drawLine(svN, '#f59e0b'); // orange: sqrt(vhat)
          drawLine(stN, '#10b981'); // green: |Δw|

          // legend
          const items = [
            ['g_t', '#9ca3af'], ['m̂_t', '#2563eb'], ['√v̂_t', '#f59e0b'], ['|Δw_t|', '#10b981']
          ];
          let lx = 48, ly = 14;
          items.forEach(([name, col]) => {
            ctx.fillStyle = col; ctx.fillRect(lx, ly - 8, 12, 4);
            ctx.fillStyle = '#374151'; ctx.font = '10px Inter, sans-serif';
            ctx.fillText(name, lx + 16, ly);
            lx += 64;
          });
        },

        recompute() { this.compute(); this.draw(); },
        init() { this.compute(); this.draw(); }
      }
    }
  </script>

  <script>
    function lipschitzDemo() {
      return {
        layers: [
          { sigma: 1.10, Lphi: 1.00 },
          { sigma: 0.95, Lphi: 1.00 },
          { sigma: 1.05, Lphi: 1.00 },
        ],
        get L() { return this.layers.length; },
        forward: 1.0,
        log10f: 0.0,
        zoneText: 'near 1 → OK',
        zoneClass: 'bg-emerald-100 text-emerald-700',

        update() {
          const prodSigma = this.layers.reduce((a, ly) => a * ly.sigma, 1.0);
          const prodLphi = this.layers.slice(0, this.layers.length - 1)
            .reduce((a, ly) => a * ly.Lphi, 1.0); // last layer often no φ
          this.forward = prodSigma * prodLphi;
          this.log10f = Math.log10(Math.max(1e-12, this.forward));
          // zones
          if (this.log10f > 0.3) { // > ~2x
            this.zoneText = '↑ exploding risk'; this.zoneClass = 'bg-rose-100 text-rose-700';
          } else if (this.log10f < -0.3) { // < ~1/2x
            this.zoneText = '↓ vanishing risk'; this.zoneClass = 'bg-amber-100 text-amber-700';
          } else {
            this.zoneText = 'near 1 → OK'; this.zoneClass = 'bg-emerald-100 text-emerald-700';
          }
        },

        add() { this.layers.push({ sigma: 1.00, Lphi: 1.00 }); this.update(); },
        remove() { this.layers.pop(); this.update(); },
        randomize() {
          for (const ly of this.layers) {
            ly.sigma = 0.9 + Math.random() * 0.2; // [0.9,1.1]
            ly.Lphi = 0.95 + Math.random() * 0.1; // [0.95,1.05]
          }
          this.update();
        },

        init() { this.update(); }
      }
    }
  </script>

  <script>
    function residualODE() {
      return {
        lamR: -0.5, lamI: 0.0, h: 0.5, K: 30,
        modLam: 0, modResid: 0, plainK: 0, residK: 0, stable: true,

        run() {
          // |lambda| and |1 + h*lambda| for complex lambda = a + i b
          const a = this.lamR, b = this.lamI;
          const modLam = Math.hypot(a, b);
          const onePlusReal = 1 + this.h * a;
          const onePlusImag = this.h * b;
          const modResid = Math.hypot(onePlusReal, onePlusImag);

          this.modLam = modLam;
          this.modResid = modResid;

          // amplification over K layers
          this.plainK = Math.pow(Math.max(1e-12, modLam), this.K);
          this.residK = Math.pow(Math.max(1e-12, modResid), this.K);

          // Euler stability: |1 + h*lambda| < 1
          this.stable = (modResid < 1.0);
        },

        init() { this.run(); }
      }
    }
  </script>
</body>
</html>
