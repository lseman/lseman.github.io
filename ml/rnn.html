<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>RNNs — Illustrated Cheatsheet (LSTM · GRU · Encoder–Decoder · Attention)</title>
  <!-- Tailwind CSS -->
  <script src="https://cdn.tailwindcss.com"></script>
  <!-- MathJax (v3) -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [["$", "$"], ["\\(", "\\)"]], displayMath: [["$$", "$$"], ["\\[", "\\]"]] },
      chtml: { linebreaks: { automatic: false }, matchFontHeight: true },
      options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
  <style>
    html {
      scroll-behavior: smooth
    }

    .lbl {
      font-size: 13px;
      fill: #374151;
      font-weight: 500
    }

    .tip {
      font-size: 12px;
      color: #64748b
    }

    .gate {
      fill: #fefefe;
      stroke: #6366f1;
      stroke-width: 2;
      rx: 12
    }

    .wire {
      stroke: #64748b;
      stroke-width: 2.5;
      fill: none
    }

    .rail {
      stroke: #1e293b;
      stroke-width: 3;
      fill: none
    }

    .op {
      fill: #f8fafc;
      stroke: #6366f1;
      stroke-width: 2
    }

    .inDot {
      fill: #dbeafe;
      stroke: #3b82f6;
      stroke-width: 2
    }

    .stateDot {
      fill: #dcfce7;
      stroke: #22c55e;
      stroke-width: 2
    }

    .peephole {
      stroke: #94a3b8;
      stroke-dasharray: 6 4;
      stroke-width: 2;
      fill: none
    }

    .card {
      backdrop-filter: blur(4px)
    }

    .sigmoid {
      fill: #fef3c7;
      stroke: #f59e0b;
      stroke-width: 2
    }

    .tanh-gate {
      fill: #f0f9ff;
      stroke: #0ea5e9;
      stroke-width: 2
    }

    .multiply {
      fill: #fef2f2;
      stroke: #ef4444;
      stroke-width: 2
    }

    .add {
      fill: #f0fdf4;
      stroke: #22c55e;
      stroke-width: 2
    }
  </style>
</head>

<body class="min-h-screen text-slate-800 bg-slate-50">

  <!-- ======= Header ======= -->
  <header class="bg-gradient-to-tr from-indigo-50 via-emerald-50 to-cyan-50 border-b border-slate-200">
    <div class="max-w-7xl mx-auto px-6 py-12 lg:py-16">
      <div class="flex flex-col lg:flex-row items-center gap-8">
        <div class="flex-1">
          <h1 class="text-4xl md:text-5xl font-extrabold tracking-tight text-slate-900">RNNs — <span
              class="text-indigo-600">Illustrated Cheatsheet</span></h1>
          <p class="mt-3 text-lg md:text-xl text-slate-700 max-w-2xl">Single-file reference for <b>LSTM</b>, <b>GRU</b>,
            and <b>Encoder→Decoder</b> (with attention). Compact textbook-style schematics + equations.</p>
          <div class="mt-5 flex flex-wrap gap-3">
            <a href="#lstm"
              class="px-5 py-3 rounded-xl bg-indigo-600 text-white font-semibold shadow hover:bg-indigo-700">LSTM</a>
            <a href="#gru"
              class="px-5 py-3 rounded-xl bg-white ring-1 ring-slate-200 shadow-sm font-semibold hover:bg-slate-50">GRU</a>
            <a href="#encdec"
              class="px-5 py-3 rounded-xl bg-white ring-1 ring-slate-200 shadow-sm font-semibold hover:bg-slate-50">Encoder/Decoder</a>
            <a href="#attention"
              class="px-5 py-3 rounded-xl bg-white ring-1 ring-slate-200 shadow-sm font-semibold hover:bg-slate-50">Attention</a>
          </div>
          <p class="mt-4 text-sm text-slate-500">MathJax • Tailwind • Pure SVG diagrams • No build step</p>
        </div>
      </div>
    </div>
  </header>

  <section id="rnn" class="py-10 bg-slate-50">
    <div class="max-w-7xl mx-auto px-6 grid lg:grid-cols-1 gap-6">
      <div class="flex items-center gap-3">
        <div class="w-2 h-8 bg-gradient-to-b from-blue-400 to-blue-600 rounded"></div>
        <hq class="text-2xl font-bold text-slate-900">Vanilla RNN</h2>
      </div>
      <svg viewBox="0 0 960 320" xmlns="http://www.w3.org/2000/svg" role="img" aria-labelledby="title desc">
        <title id="title">Unrolled Recurrent Neural Network (RNN)</title>
        <desc id="desc">An RNN unrolled across time steps t-1, t, and t+1, showing inputs x, hidden states h carrying
          memory forward, and outputs y.</desc>

        <!-- defs -->
        <defs>
          <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8"
            orient="auto-start-reverse">
            <path d="M 0 0 L 10 5 L 0 10 z" fill="#334155" />
          </marker>
          <style>
            .cell {
              fill: #ffffff;
              stroke: #334155;
              stroke-width: 2;
              rx: 14;
            }

            .io {
              fill: #e2e8f0;
              stroke: #334155;
              stroke-width: 2;
            }

            .thin {
              stroke: #94a3b8;
              stroke-width: 1.5;
            }

            .heavy {
              stroke: #334155;
              stroke-width: 2;
            }

            .dashed {
              stroke-dasharray: 6 6;
            }

            .label {
              font: 500 14px/1.2 ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, "Apple Color Emoji", "Segoe UI Emoji";
              fill: #334155;
            }

            .title {
              font: 600 16px/1.2 ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
              fill: #0f172a;
            }

            .small {
              font: 12px/1.2 ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
              fill: #475569;
            }

            .legend {
              font: 12px/1.2 ui-sans-serif, system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial;
              fill: #334155;
            }
          </style>
        </defs>

        <!-- timeline guide -->
        <g transform="translate(0,20)">
          <text x="180" y="0" class="title">RNN unrolled in time</text>
        </g>

        <!-- columns x/h/y positions -->
        <!-- t-1 column -->
        <g id="t_minus_1" transform="translate(80,70)">
          <!-- input x_{t-1} -->
          <circle class="io" cx="0" cy="130" r="26" />
          <text class="label" x="0" y="134" text-anchor="middle">x<tspan baseline-shift="sub">t−1</tspan></text>

          <!-- cell -->
          <rect class="cell" x="120" y="90" width="110" height="80" />
          <text class="label" x="175" y="125" text-anchor="middle">RNN cell</text>
          <text class="small" x="175" y="144" text-anchor="middle">h<tspan baseline-shift="sub">t−1</tspan> → h<tspan
              baseline-shift="sub">t</tspan></text>

          <!-- output y_{t-1} -->
          <circle class="io" cx="330" cy="130" r="26" />
          <text class="label" x="330" y="134" text-anchor="middle">y<tspan baseline-shift="sub">t−1</tspan></text>

          <!-- arrows: x_{t-1} -> cell, cell -> y_{t-1} -->
          <line class="heavy" x1="26" y1="130" x2="120" y2="130" marker-end="url(#arrow)" />
          <line class="heavy" x1="230" y1="130" x2="304" y2="130" marker-end="url(#arrow)" />

          <!-- time label -->
          <text class="small" x="175" y="190" text-anchor="middle">t−1</text>
        </g>

        <!-- t column -->
        <g id="t" transform="translate(80,70)">
          <!-- input x_t -->
          <circle class="io" cx="420" cy="130" r="26" />
          <text class="label" x="420" y="134" text-anchor="middle">x<tspan baseline-shift="sub">t</tspan></text>

          <!-- cell -->
          <rect class="cell" x="540" y="90" width="110" height="80" />
          <text class="label" x="595" y="125" text-anchor="middle">RNN cell</text>
          <text class="small" x="595" y="144" text-anchor="middle">h<tspan baseline-shift="sub">t−1</tspan>, x<tspan
              baseline-shift="sub">t</tspan> → h<tspan baseline-shift="sub">t</tspan></text>

          <!-- output y_t -->
          <circle class="io" cx="750" cy="130" r="26" />
          <text class="label" x="750" y="134" text-anchor="middle">y<tspan baseline-shift="sub">t</tspan></text>

          <!-- arrows: x_t -> cell, cell -> y_t -->
          <line class="heavy" x1="446" y1="130" x2="540" y2="130" marker-end="url(#arrow)" />
          <line class="heavy" x1="650" y1="130" x2="724" y2="130" marker-end="url(#arrow)" />

          <!-- time label -->
          <text class="small" x="595" y="190" text-anchor="middle">t</text>
        </g>

        <!-- t+1 column (faded future) -->
        <g id="t_plus_1" transform="translate(80,70)" opacity="0.55">
          <!-- input -->
          <circle class="io" cx="840" cy="130" r="26" />
          <text class="label" x="840" y="134" text-anchor="middle">x<tspan baseline-shift="sub">t+1</tspan></text>

          <!-- cell -->
          <rect class="cell" x="960" y="90" width="110" height="80" />
          <text class="label" x="1015" y="125" text-anchor="middle">RNN cell</text>
          <text class="small" x="1015" y="144" text-anchor="middle">h<tspan baseline-shift="sub">t</tspan> → h<tspan
              baseline-shift="sub">t+1</tspan></text>

          <!-- output -->
          <circle class="io" cx="1170" cy="130" r="26" />
          <text class="label" x="1170" y="134" text-anchor="middle">y<tspan baseline-shift="sub">t+1</tspan></text>

          <line class="heavy" x1="866" y1="130" x2="960" y2="130" marker-end="url(#arrow)" />
          <line class="heavy" x1="1070" y1="130" x2="1144" y2="130" marker-end="url(#arrow)" />

          <text class="small" x="1015" y="190" text-anchor="middle">t+1</text>
        </g>

        <!-- recurrent (memory) connections: h_{t-1} -> h_t -> h_{t+1} -->
        <!-- from left cell to middle cell -->
        <g transform="translate(80,70)">
          <path class="heavy" d="M 230 90 C 260 40, 510 40, 540 90" fill="none" marker-end="url(#arrow)" />
          <text class="small" x="385" y="38" text-anchor="middle">hidden state carries memory: h<tspan
              baseline-shift="sub">t−1</tspan> → h<tspan baseline-shift="sub">t</tspan></text>
        </g>

        <!-- from middle cell to right (future, dashed hint) -->
        <g transform="translate(80,70)" opacity="0.8">
          <path class="thin dashed" d="M 650 90 C 680 40, 930 40, 960 90" fill="none" marker-end="url(#arrow)" />
          <text class="small" x="805" y="26" text-anchor="middle">h<tspan baseline-shift="sub">t</tspan> → h<tspan
              baseline-shift="sub">t+1</tspan></text>
        </g>

        <!-- small legend -->
        <g transform="translate(80,300)">
          <rect x="0" y="-16" width="330" height="28" rx="6" fill="#f1f5f9" stroke="#cbd5e1" />
          <circle class="io" cx="16" cy="-2" r="8" />
          <text class="legend" x="32" y="1">input x<tspan baseline-shift="sub">t</tspan> / output y<tspan
              baseline-shift="sub">t</tspan></text>
          <rect class="cell" x="188" y="-15" width="22" height="22" rx="5" />
          <text class="legend" x="214" y="1">RNN cell (updates h<tspan baseline-shift="sub">t</tspan>)</text>
        </g>
      </svg>
    </div>
  </section>


  <!-- ======= Exploding / Vanishing Gradients (Eigenvalue Proof) ======= -->
  <section id="rnn-gradients" class="py-10 bg-slate-50">

    <div class="max-w-7xl mx-auto px-6 grid lg:grid-cols-2 gap-6">

      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h2 class="text-2xl font-bold">Why do vanilla RNN gradients explode or vanish?</h2>
        <p class="text-sm text-slate-700 mt-2">
          Consider the (vanilla) RNN
          \[
          h_t = \phi(W_h h_{t-1} + W_x x_t + b), \qquad y_t = g(h_t),
          \]
          with a scalar loss \(L=L(h_T)\). Backpropagation yields
          \[
          \frac{\partial L}{\partial h_t}
          \;=\;\frac{\partial L}{\partial h_T}\;\prod_{k=t+1}^{T} J_k,
          \qquad
          J_k := \frac{\partial h_k}{\partial h_{k-1}}
          \;=\; D_k\,W_h,
          \]
          where \(D_k=\mathrm{diag}\big(\phi'(W_h h_{k-1}+W_x x_k+b)\big)\) is the pointwise activation Jacobian.
        </p>

        <h3 class="text-lg font-semibold mt-4">Linear case (\(\phi = \mathrm{id}\))</h3>
        <p class="text-sm text-slate-700 mt-2">
          If \(\phi\) is the identity, then \(J_k=W_h\) and
          \[
          \frac{\partial L}{\partial h_t} \;=\; \frac{\partial L}{\partial h_T}\; W_h^{\,T-t}.
          \]
          Let \(\|\cdot\|\) be an operator norm (e.g. spectral norm \(\|\cdot\|_2\)). Then
          \[
          \Big\|\tfrac{\partial L}{\partial h_t}\Big\|
          \;\le\; \Big\|\tfrac{\partial L}{\partial h_T}\Big\|\;\big\|W_h^{\,T-t}\big\|.
          \]
          Using Gelfand’s formula for the spectral radius \(\rho(W_h)\),
          \[
          \rho(W_h) \;=\; \lim_{n\to\infty} \big\|W_h^{\,n}\big\|^{1/n}.
          \]
          Hence asymptotically \(\big\|W_h^{\,n}\big\| \approx C\, \rho(W_h)^{\,n}\) (up to polynomial factors if
          \(W_h\) is defective). Therefore:
        </p>
        <ul class="mt-2 text-sm text-slate-700 list-disc ml-5">
          <li><b>If \(\rho(W_h)&lt;1\)</b>, then \(\|W_h^{\,T-t}\|\to 0\) exponentially ⇒ <b>vanishing gradients</b>.
          </li>
          <li><b>If \(\rho(W_h)&gt;1\)</b>, then \(\|W_h^{\,T-t}\|\to \infty\) exponentially ⇒ <b>exploding
              gradients</b>.</li>
          <li><b>If \(\rho(W_h)=1\)</b> and \(W_h\) is diagonalizable with unimodular eigenvalues (e.g. orthogonal),
            then norms are preserved up to constants (no systematic explosion/vanishing).</li>
        </ul>
        <details class="mt-3">
          <summary class="text-sm font-semibold text-indigo-700">Eigen-decomposition view</summary>
          <div class="text-sm text-slate-700 mt-2">
            If \(W_h = Q \Lambda Q^{-1}\) (diagonalizable), then \(W_h^{\,n}=Q\,\Lambda^n Q^{-1}\).
            Components along eigenvectors scale like \(\lambda_i^{\,n}\). Thus the largest \(|\lambda_i|\) governs
            growth/decay.
            For non-diagonalizable \(W_h\), Jordan blocks add polynomial factors in \(n\), but the exponential rate is
            still set by \(\rho(W_h)\).
          </div>
        </details>

        <h3 class="text-lg font-semibold mt-4">Nonlinear case (general \(\phi\))</h3>
        <p class="text-sm text-slate-700 mt-2">
          Recall \(J_k = D_k W_h\). For many activations, \(\|D_k\|\le \gamma\) where
          \(\gamma := \sup_z |\phi'(z)|\) (e.g., \(\gamma\le 1\) for \(\tanh\), \(\gamma\le 1\) for ReLU in active
          units, \(0\) for dead units).
          Then
          \[
          \Big\|\prod_{k=t+1}^{T} J_k\Big\|
          \;\le\; \prod_{k=t+1}^{T} \|D_k\|\,\|W_h\|
          \;\le\; (\gamma\,\|W_h\|)^{\,T-t}.
          \]
          Consequently:
        </p>
        <ul class="mt-2 text-sm text-slate-700 list-disc ml-5">
          <li>If \(\gamma\,\|W_h\|&lt;1\), gradients decay exponentially.</li>
          <li>If \(\gamma\,\|W_h\|&gt;1\), gradients can blow up exponentially.</li>
        </ul>
        <p class="text-sm text-slate-700 mt-2">
          A sharper statement uses spectral radii of the per-step Jacobians:
          if \(\rho(J_k)\le \eta_k\), then
          \[
          \Big\|\tfrac{\partial L}{\partial h_t}\Big\|
          \;\le\; \Big\|\tfrac{\partial L}{\partial h_T}\Big\|\;\prod_{k=t+1}^{T} \eta_k,
          \]
          so the exponential rate is controlled by \(\limsup_{n\to\infty}\big(\prod_{k=1}^{n}\eta_k\big)^{1/n}\).
          When inputs drive \(D_k\) near zero (saturation) or one (linear regime), the product correspondingly shrinks
          or grows.
        </p>

        <h3 class="text-lg font-semibold mt-4">Takeaways</h3>
        <ul class="mt-2 text-sm text-slate-700 list-disc ml-5">
          <li><b>Root cause:</b> backprop multiplies many Jacobians. Their dominant eigenvalues (<i>spectral radii</i>)
            set exponential growth/decay.</li>
          <li><b>Stabilizers:</b> orthogonal/identity-scaled \(W_h\) (keep \(\rho\approx 1\)), LayerNorm, gradient
            clipping, truncated BPTT.</li>
          <li><b>Structural fix:</b> LSTM/GRU add nearly-linear “carry” paths so the effective Jacobian has eigenvalues
            near 1 (controlled by gates).</li>
        </ul>
      </article>

      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h3 class="text-xl font-semibold">Formal statement & proof (compact)</h3>
        <p class="text-sm text-slate-700 mt-2">
          <b>Theorem (Linear RNN).</b> For \(h_t=W_h h_{t-1}\) and loss \(L(h_T)\),
          \(\displaystyle \frac{\partial L}{\partial h_t}= \frac{\partial L}{\partial h_T}\,W_h^{\,T-t}\).
          Let \(\rho=\rho(W_h)\) be the spectral radius. Then as \(T-t\to\infty\),
          \[
          \Big\|\tfrac{\partial L}{\partial h_t}\Big\| =
          \Theta\big(\rho^{\,T-t}\big)\quad\text{up to polynomial factors},
          \]
          hence gradients vanish if \(\rho&lt;1\), explode if \(\rho&gt;1\), and are preserved (modulo constants) if
          \(W_h\) is diagonalizable with \(\rho=1\).
        </p>
        <details class="mt-3">
          <summary class="text-sm font-semibold text-indigo-700">Proof</summary>
          <div class="text-sm text-slate-700 mt-2 space-y-2">
            <p><b>(1)</b> Chain rule gives \(\frac{\partial L}{\partial h_t}=\frac{\partial L}{\partial
              h_T}\,W_h^{\,T-t}\).</p>
            <p><b>(2)</b> By Gelfand’s formula, \(\rho=\lim_{n\to\infty}\|W_h^{\,n}\|^{1/n}\) for any operator norm.
              Thus
              for large \(n\), \(\|W_h^{\,n}\|\) behaves like \(C\,\rho^{\,n}\) up to polynomial terms (Jordan blocks).
            </p>
            <p><b>(3)</b> Combine (1) and (2) to obtain the stated asymptotics. □</p>
          </div>
        </details>

        <h3 class="text-lg font-semibold mt-4">Corollary (Nonlinear RNN)</h3>
        <p class="text-sm text-slate-700 mt-2">
          With \(J_k=D_k W_h\) and \(\|D_k\|\le\gamma\), we have
          \(\big\|\frac{\partial L}{\partial h_t}\big\|\le \big\|\frac{\partial L}{\partial
          h_T}\big\|(\gamma\|W_h\|)^{\,T-t}\).
          Hence the product of per-step Lipschitz constants controls the gradient’s exponential rate.
        </p>

        <h4 class="text-lg font-semibold mt-4">Practical implications</h4>
        <ul class="mt-2 text-sm text-slate-700 list-disc ml-5">
          <li>Initialize \(W_h\) orthogonal (or unitary) to set \(\rho(W_h)\approx 1\); add small input scale.</li>
          <li>Use LayerNorm/BatchNorm on preactivations to constrain \(\|D_k\|\).</li>
          <li>Clip global gradient norm (e.g., 1–5) and use truncated BPTT.</li>
          <li>Prefer gated RNNs (LSTM/GRU): gates learn effective eigenvalues near 1 along memory paths.</li>
        </ul>
      </article>
    </div>
  </section>


  <!-- ======= LSTM ======= -->
  <section id="lstm" class="py-10">

    <div class="max-w-7xl mx-auto px-6 grid lg:grid-cols-1 gap-6">
      <div class="flex items-center gap-3">
        <div class="w-2 h-8 bg-gradient-to-b from-blue-400 to-blue-600 rounded"></div>
        <hq class="text-2xl font-bold text-slate-900">LSTM</h2>
      </div>

      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h2 class="text-2xl font-bold">LSTM — Equations</h2>
        <div class="text-sm text-slate-700 leading-7 mt-2">
          $$\begin{aligned}
          i_t&=\sigma(W_i x_t + U_i h_{t-1} + b_i),\quad
          f_t=\sigma(W_f x_t + U_f h_{t-1} + b_f)\\
          o_t&=\sigma(W_o x_t + U_o h_{t-1} + b_o),\quad
          \tilde c_t=\tanh(W_c x_t + U_c h_{t-1} + b_c)\\
          c_t&=f_t\odot c_{t-1} + i_t\odot \tilde c_t,\quad
          h_t=o_t\odot \tanh(c_t)
          \end{aligned}$$
          <ul class="mt-3 list-disc ml-5">
            <li><b>Peepholes</b>: add $+V_i\odot c_{t-1}$ etc. inside gate logits (often dotted in diagrams).</li>
            <li><b>LayerNorm-LSTM</b>: apply LayerNorm to each affine pre-activation; stabilize training on long seqs.
            </li>
            <li><b>Forget-gate bias</b>: initialize $b_f \approx 1\sim2$ to encourage remembering early on.</li>
          </ul>
        </div>
        <h3 class="text-lg font-semibold mt-4">PyTorch init snippet</h3>
        <pre class="text-sm code bg-slate-900 text-slate-100 p-4 rounded-xl overflow-x-auto"><code>def init_lstm_(lstm, forget_bias=1.0):
    for name, p in lstm.named_parameters():
        if "weight_hh" in name:
            torch.nn.init.orthogonal_(p)     # recurrent orthogonal
        elif "weight_ih" in name:
            torch.nn.init.xavier_uniform_(p) # input xavier
        elif "bias" in name:
            p.data.fill_(0.)
            # PyTorch biases are [i, f, g, o] by default
            H = p.shape[0] // 4
            p.data[H:2*H].fill_(forget_bias)</code></pre>
      </article>



      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h3 class="text-xl font-semibold mb-2">LSTM — Gradient Flow</h3>
        <p class="text-sm text-slate-700 mt-2">The LSTM mitigates vanishing gradients via its cell state \( c_t \),
          which provides a near-linear “carry” path controlled by the forget gate \( f_t \). When \( f_t \approx 1 \),
          the Jacobian \( \frac{\partial c_t}{\partial c_{t-1}} \approx f_t \) is near 1, preserving gradients across
          time steps.</p>
        <div class="text-sm text-slate-700 mt-2">
          <strong>Key Equation:</strong>
          \[ c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t, \quad \text{where} \quad \frac{\partial c_t}{\partial
          c_{t-1}} \approx f_t \]
        </div>
        <svg viewBox="0 0 600 300" class="w-full h-auto mt-4 tex2jax_ignore" xmlns="http://www.w3.org/2000/svg"
          role="img" aria-labelledby="lstm-grad-title lstm-grad-desc">
          <title id="lstm-grad-title">LSTM Cell State Gradient Flow</title>
          <desc id="lstm-grad-desc">Diagram showing the LSTM cell state carry path with forget gate controlling gradient
            flow, annotated with Jacobian terms.</desc>
          <!-- Definitions -->
          <defs>
            <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8"
              orient="auto-start-reverse">
              <path d="M 0 0 L 10 5 L 0 10 z" fill="#334155" />
            </marker>
          </defs>
          <!-- Cell state path -->
          <path class="rail" d="M 50 140 H 550" marker-end="url(#arrow)" />
          <text x="340" y="110" class="lbl" text-anchor="middle">Cell state c₍t₋₁₎ → c₍t₎</text>
          <!-- Forget gate -->
          <rect x="220" y="60" width="60" height="40" class="sigmoid" />
          <text x="250" y="85" class="lbl" text-anchor="middle">f₍t₎</text>
          <!-- Multiplication operation -->
          <circle class="multiply" cx="320" cy="140" r="20" />
          <text x="320" y="145" class="lbl" text-anchor="middle">⊙</text>
          <!-- Input from forget gate to multiplication -->
          <path class="wire" d="M 250 100 V 140" marker-end="url(#arrow)" />
          <!-- Annotations -->
          <text x="420" y="170" class="tip" text-anchor="middle">Jacobian ∂c₍t₎/∂c₍t₋₁₎ ≈ f₍t₎</text>
          <text x="420" y="190" class="tip" text-anchor="middle">f₍t₎ ≈ 1: Near-linear gradient</text>
          <!-- Legend -->
          <g transform="translate(350, 220)">
            <path class="rail" d="M 0 0 H 40" />
            <text x="50" y="5" class="legend">Cell state carry path</text>
            <rect x="0" y="20" width="20" height="20" class="sigmoid" />
            <text x="30" y="35" class="legend">Forget gate f₍t₎ = σ(·)</text>
            <circle cx="0" cy="55" r="10" class="multiply" />
            <text x="20" y="60" class="legend">Element-wise multiplication</text>
          </g>
        </svg>
        <p class="tip mt-2">The forget gate \( f_t \) (sigmoid) modulates the cell state, keeping gradients stable when
          \( f_t \approx 1 \), unlike vanilla RNNs where repeated matrix multiplications cause exponential decay or
          growth.</p>
      </article>



      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h3 class="text-xl font-semibold mb-2">LSTM — Fully Wired Cell</h3>
        <?xml version="1.0" encoding="UTF-8" standalone="no"?>
        <img src="lstm_cell_diagram.svg" alt="LSTM Fully Wired Cell Diagram" class="w-full h-auto">

        <p class="tip mt-2">Information flows: Cell state (thick line) carries long-term memory, gates control
          information flow.</p>
      </article>
    </div>
  </section>

  <!-- ======= GRU ======= -->
  <section id="gru" class="py-10">
    <div class="max-w-7xl mx-auto px-6 grid lg:grid-cols-1 gap-6">
        <div class="flex items-center gap-3">
          <div class="w-2 h-8 bg-gradient-to-b from-blue-400 to-blue-600 rounded"></div>
          <hq class="text-2xl font-bold text-slate-900">GRU</h2>
        </div>

        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h2 class="text-2xl font-bold">GRU — Equations</h2>
          <div class="text-sm text-slate-700 leading-7 mt-2 math-nowrap">
            $$\begin{aligned}
            z_t &= \sigma(W_z x_t + U_z h_{t-1}), & r_t &= \sigma(W_r x_t + U_r h_{t-1}) \\
            \tilde h_t &= \tanh(W_h x_t + U_h (r_t \odot h_{t-1})), & h_t &= (1-z_t)\odot h_{t-1} + z_t\odot\tilde h_t
            \end{aligned}$$
          </div>
          <ul class="mt-3 text-sm text-slate-700 list-disc ml-5">
            <li>No explicit cell state; a single hidden state $h_t$.</li>
            <li>Often trains faster than LSTM with similar quality for many tasks.</li>
          </ul>
        </article>

        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h3 class="text-xl font-semibold mb-2">GRU — Fully Wired Cell</h3>
          <img src="gru_cell_diagram.svg" alt="LSTM Fully Wired Cell Diagram" class="w-full h-auto">
          <p class="tip mt-2">Reset gate controls how much past information to use; update gate balances old vs new
            information.</p>
        </article>
      </div>
  </section>

  <!-- ======= Encoder / Decoder (Upgraded) ======= -->
  <section id="encdec" class="py-10 bg-slate-50">
    <div class="max-w-7xl mx-auto px-6 grid xl:grid-cols-1 lg:grid-cols-1 gap-6">
        <div class="flex items-center gap-3">
          <div class="w-2 h-8 bg-gradient-to-b from-blue-400 to-blue-600 rounded"></div>
          <hq class="text-2xl font-bold text-slate-900">Architectures</h2>
        </div>

        <!-- ========== 1) Vanilla Seq2Seq ========== -->
        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h2 class="text-2xl font-bold">Encoder→Decoder (Vanilla)</h2>
          <p class="text-sm text-slate-700 mt-2">
            Fixed-length context vector (bottleneck) passed from encoder to decoder.
          </p>
          <svg viewBox="0 0 820 300" class="w-full h-auto mt-4">
            <defs>
              <marker id="arrC" markerWidth="8" markerHeight="8" refX="7" refY="4" orient="auto"
                markerUnits="strokeWidth">
                <path d="M0,0 L0,8 L8,4 z" fill="#64748b" />
              </marker>
              <linearGradient id="encGrad" x1="0%" y1="0%" x2="100%" y2="100%">
                <stop offset="0%" style="stop-color:#fef3c7" />
                <stop offset="100%" style="stop-color:#fde68a" />
              </linearGradient>
              <linearGradient id="decGrad" x1="0%" y1="0%" x2="100%" y2="100%">
                <stop offset="0%" style="stop-color:#dbeafe" />
                <stop offset="100%" style="stop-color:#bfdbfe" />
              </linearGradient>
            </defs>

            <!-- Inputs -->
            <g class="inDot">
              <circle cx="80" cy="36" r="10" />
              <circle cx="200" cy="36" r="10" />
              <circle cx="320" cy="36" r="10" />
            </g>
            <text x="100" y="56" class="lbl" text-anchor="middle">x₁</text>
            <text x="220" y="56" class="lbl" text-anchor="middle">x₂</text>
            <text x="340" y="56" class="lbl" text-anchor="middle">x₃</text>

            <!-- Encoder cells -->
            <g>
              <rect x="40" y="72" width="80" height="52" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2"
                rx="10" />
              <rect x="160" y="72" width="80" height="52" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2"
                rx="10" />
              <rect x="280" y="72" width="80" height="52" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2"
                rx="10" />
              <text x="80" y="103" class="lbl" text-anchor="middle">Enc</text>
              <text x="200" y="103" class="lbl" text-anchor="middle">Enc</text>
              <text x="320" y="103" class="lbl" text-anchor="middle">Enc</text>
            </g>

            <!-- Encoder wiring -->
            <g class="wire" marker-end="url(#arrC)">
              <path d="M80,46 V72" />
              <path d="M200,46 V72" />
              <path d="M320,46 V72" />
              <path d="M120,98 H160" />
              <path d="M240,98 H280" />
            </g>

            <!-- Context -->
            <rect x="430" y="72" width="110" height="52" class="gate" rx="10" />
            <text x="485" y="103" class="lbl" text-anchor="middle">Context</text>
            <path class="wire" marker-end="url(#arrC)" d="M360,98 H430" />

            <!-- Decoder cells (right-to-left for time) -->
            <g>
              <rect x="690" y="160" width="80" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <rect x="570" y="160" width="80" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <rect x="450" y="160" width="80" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <text x="730" y="191" class="lbl" text-anchor="middle">Dec</text>
              <text x="610" y="191" class="lbl" text-anchor="middle">Dec</text>
              <text x="490" y="191" class="lbl" text-anchor="middle">Dec</text>
            </g>

            <!-- Decoder wiring -->
            <g class="wire" marker-end="url(#arrC)">
              <path d="M485,124 V160" />
              <path d="M530,186 H570" />
              <path d="M650,186 H690" />
              <path d="M770,186 H790" />
            </g>

            <!-- Output token -->
            <g class="stateDot">
              <circle cx="800" cy="186" r="10" />
            </g>
            <text x="800" y="206" class="lbl" text-anchor="middle">ŷ</text>

            <!-- Labels -->
            <text x="200" y="140" class="text-sm font-semibold" text-anchor="middle" fill="#f59e0b">ENCODER</text>
            <text x="610" y="230" class="text-sm font-semibold" text-anchor="middle" fill="#3b82f6">DECODER</text>
          </svg>
          <p class="tip mt-2">Bottleneck risk: long inputs must compress into a single vector.</p>
        </article>

              <!-- ========== 3) Decoder Input Modes: Teacher Forcing vs Free-Running ========== -->
        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h2 class="text-2xl font-bold">Decoder Input Modes</h2>
          <p class="text-sm text-slate-700 mt-2">Training with <b>teacher forcing</b> vs inference with
            <b>free-running</b> (autoregressive).
          </p>

          <!-- Teacher Forcing -->
          <h3 class="text-lg font-semibold mt-4">Teacher Forcing (Training)</h3>
          <svg viewBox="0 0 820 200" class="w-full h-auto mt-2">
            <defs>
              <marker id="arrTF" markerWidth="8" markerHeight="8" refX="7" refY="4" orient="auto"
                markerUnits="strokeWidth">
                <path d="M0,0 L0,8 L8,4 z" fill="#fb923c" />
              </marker>
              <marker id="arrDec" markerWidth="8" markerHeight="8" refX="7" refY="4" orient="auto"
                markerUnits="strokeWidth">
                <path d="M0,0 L0,8 L8,4 z" fill="#64748b" />
              </marker>
            </defs>

            <!-- Ground-truth tokens -->
            <g>
              <circle cx="150" cy="36" r="10" fill="#fb923c" /><text x="150" y="56" class="lbl"
                text-anchor="middle">y₀</text>
              <circle cx="270" cy="36" r="10" fill="#fb923c" /><text x="270" y="56" class="lbl"
                text-anchor="middle">y₁</text>
              <circle cx="390" cy="36" r="10" fill="#fb923c" /><text x="390" y="56" class="lbl"
                text-anchor="middle">y₂</text>
            </g>

            <!-- Decoder cells -->
            <g>
              <rect x="240" y="90" width="100" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <rect x="360" y="90" width="100" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <rect x="480" y="90" width="100" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <text x="290" y="121" class="lbl" text-anchor="middle">Dec t=1</text>
              <text x="410" y="121" class="lbl" text-anchor="middle">Dec t=2</text>
              <text x="530" y="121" class="lbl" text-anchor="middle">Dec t=3</text>
            </g>

            <!-- Teacher forcing arrows (dashed orange from y_{t-1}) -->
            <g class="wire dash" marker-end="url(#arrTF)" stroke="#fb923c">
              <path d="M150,46 C190,70 260,70 290,90" />
              <path d="M270,46 C310,70 380,70 410,90" />
              <path d="M390,46 C430,70 500,70 530,90" />
            </g>

            <!-- Outputs -->
            <g class="lbl">
              <text x="350" y="136" text-anchor="middle">ŷ₁</text>
              <text x="470" y="136" text-anchor="middle">ŷ₂</text>
              <text x="590" y="136" text-anchor="middle">ŷ₃</text>
            </g>

            <!-- Forward decode connections -->
            <g class="wire" marker-end="url(#arrDec)">
              <path d="M340,116 H360" />
              <path d="M460,116 H480" />
              <path d="M580,116 H600" />
            </g>

            <text x="410" y="180" class="text-sm font-medium" text-anchor="middle" fill="#fb923c">
              Use ground-truth previous token as decoder input → faster/steadier training.
            </text>
          </svg>

          <!-- Free-Running -->
          <h3 class="text-lg font-semibold mt-6">Free-Running (Inference)</h3>
          <svg viewBox="0 0 820 200" class="w-full h-auto mt-2">
            <defs>
              <marker id="arrFR" markerWidth="8" markerHeight="8" refX="7" refY="4" orient="auto"
                markerUnits="strokeWidth">
                <path d="M0,0 L0,8 L8,4 z" fill="#3b82f6" />
              </marker>
            </defs>

            <!-- Start token -->
            <g>
              <circle cx="150" cy="36" r="10" fill="#0ea5e9" /><text x="150" y="56" class="lbl"
                text-anchor="middle">&lt;bos&gt;</text>
            </g>

            <!-- Decoder cells -->
            <g>
              <rect x="240" y="90" width="100" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <rect x="360" y="90" width="100" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <rect x="480" y="90" width="100" height="52" fill="url(#decGrad)" stroke="#3b82f6" stroke-width="2"
                rx="10" />
              <text x="290" y="121" class="lbl" text-anchor="middle">Dec t=1</text>
              <text x="410" y="121" class="lbl" text-anchor="middle">Dec t=2</text>
              <text x="530" y="121" class="lbl" text-anchor="middle">Dec t=3</text>
            </g>

            <!-- Autoregressive feedback (solid blue from previous prediction) -->
            <g class="wire" marker-end="url(#arrFR)" stroke="#3b82f6">
              <path d="M150,6 C190,70 260,70 290,90" />
              <path d="M340,120 C350,116 360,106 350,76" />
              <path d="M460,116 C470,116 480,106 470,76" />
            </g>

            <!-- Predictions -->
            <g class="lbl">
              <text x="350" y="66" text-anchor="middle">ŷ₁</text>
              <text x="470" y="66" text-anchor="middle">ŷ₂</text>
              <text x="600" y="96" text-anchor="middle">ŷ₃</text>
            </g>

            <!-- Forward connections -->
            <g class="wire" marker-end="url(#arrFR)">
              <path d="M340,116 H360" />
              <path d="M460,116 H480" />
              <path d="M580,116 H600" />
            </g>

            <text x="410" y="180" class="text-sm font-medium" text-anchor="middle" fill="#3b82f6">
              Feed previous prediction back as next input → exposure bias may appear.
            </text>
          </svg>

          <!-- Legend -->
          <div class="mt-4 text-xs text-slate-600 grid grid-cols-2 gap-3">
            <div class="flex items-center gap-2">
              <span class="inline-block w-4 h-2 rounded" style="background:#fde68a"></span>
              <span>Encoder / hₜ</span>
            </div>
            <div class="flex items-center gap-2">
              <span class="inline-block w-4 h-2 rounded" style="background:#bfdbfe"></span>
              <span>Decoder / sₜ</span>
            </div>
            <div class="flex items-center gap-2">
              <span class="inline-block w-4 h-0.5 bg-orange-400 border-b border-orange-400"
                style="border-style:dashed"></span>
              <span>Teacher-forcing input</span>
            </div>
            <div class="flex items-center gap-2">
              <span class="inline-block w-4 h-0.5 bg-blue-500"></span>
              <span>Autoregressive feedback</span>
            </div>
          </div>
        </article>

        <!-- ========== 2) Attention (Improved) ========== -->
         

<section id="attention" class="py-10 bg-slate-50">
  <div class="max-w-7xl mx-auto px-10 grid lg:grid-cols-1 gap-10">
    <!-- Subtitle Block -->
      <div class="flex items-center gap-3">
        <div class="w-2 h-8 bg-gradient-to-b from-purple-400 to-purple-600 rounded"></div>
        <h2 class="text-2xl font-bold text-slate-900">Attention Mechanism</h2>
      </div>
    <!-- Block 1: Attention Overview -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
      <h3 class="text-xl font-semibold mb-2">1. Attention Overview</h3>
      <p class="text-sm text-slate-700 mt-2">Attention allows the decoder to focus on relevant encoder states by computing a weighted sum of values \( V \) based on a query \( q_t \) and keys \( K \). The result is a context vector \( c_t \) used for decoding.</p>
      <div class="text-sm text-slate-700 mt-2">
        <strong>Key Equation:</strong>
        \[ \mathrm{Attn}(q_t, K, V) = \mathrm{softmax}\left(\frac{q_t K^\top}{\sqrt{d}}\right) V \]
      </div>
      <svg viewBox="0 0 700 250" class="w-full h-auto mt-4 tex2jax_ignore" xmlns="http://www.w3.org/2000/svg" role="img" aria-labelledby="attn-overview-title attn-overview-desc">
        <title id="attn-overview-title">Attention Mechanism Overview</title>
        <desc id="attn-overview-desc">Diagram showing the flow of query, keys, and values through attention to produce a context vector.</desc>
        <defs>
          <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
            <path d="M 0 0 L 10 5 L 0 10 z" fill="#334155"/>
          </marker>
          <linearGradient id="encGrad" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#fef3c7"/><stop offset="100%" style="stop-color:#fde68a"/>
          </linearGradient>
          <linearGradient id="decGrad" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#e9d5ff"/><stop offset="100%" style="stop-color:#c4b5fd"/>
          </linearGradient>
        </defs>
        <!-- Encoder states (keys/values) -->
        <g>
          <rect x="50" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="150" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="250" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <text x="80" y="65" class="lbl" text-anchor="middle">k₁, v₁</text>
          <text x="180" y="65" class="lbl" text-anchor="middle">k₂, v₂</text>
          <text x="280" y="65" class="lbl" text-anchor="middle">k₃, v₃</text>
        </g>
        <!-- Query -->
        <rect x="150" y="150" width="60" height="40" fill="url(#decGrad)" stroke="#a855f7" stroke-width="2" rx="10"/>
        <text x="180" y="175" class="lbl" text-anchor="middle">q₍t₎</text>
        <!-- Attention computation -->
        <rect x="350" y="90" width="80" height="60" class="op"/>
        <text x="390" y="125" class="lbl" text-anchor="middle">Attention</text>
        <!-- Context vector -->
        <rect x="500" y="90" width="60" height="40" class="op"/>
        <text x="530" y="115" class="lbl" text-anchor="middle">c₍t₎</text>
        <!-- Connections -->
        <g class="wire" marker-end="url(#arrow)">
          <path d="M 110 60 C 180 60 300 90 350 90"/>
          <path d="M 210 60 C 280 60 320 90 350 90"/>
          <path d="M 310 60 C 330 60 340 80 350 90"/>
          <path d="M 180 190 C 250 190 300 150 350 150"/>
          <path d="M 430 120 H 500"/>
        </g>
        <text x="390" y="200" class="tip" text-anchor="middle">Weighted sum of vᵢ based on q₍t₎ and kᵢ</text>
      </svg>
      <p class="tip mt-2">Attention dynamically selects relevant encoder states, avoiding the bottleneck of fixed context vectors.</p>
    </article>
    <!-- Block 2: Scoring and Softmax -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
      <h3 class="text-xl font-semibold mb-2">2. Scoring and Softmax</h3>
      <p class="text-sm text-slate-700 mt-2">The attention mechanism computes scores by comparing the query \( q_t \) with each key \( k_i \), then applies softmax to get weights \( \alpha_{t,i} \).</p>
      <div class="text-sm text-slate-700 mt-2">
        <strong>Dot-Product Score:</strong>
        \[ \alpha_{t,i} = \mathrm{softmax}_i\left(\frac{q_t^\top k_i}{\sqrt{d}}\right) \]
      </div>
      <svg viewBox="0 0 700 200" class="w-full h-auto mt-4 tex2jax_ignore" xmlns="http://www.w3.org/2000/svg" role="img" aria-labelledby="attn-score-title attn-score-desc">
        <title id="attn-score-title">Attention Scoring and Softmax</title>
        <desc id="attn-score-desc">Diagram showing the computation of attention scores and softmax weights.</desc>
        <defs>
          <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
            <path d="M 0 0 L 10 5 L 0 10 z" fill="#334155"/>
          </marker>
          <linearGradient id="scoreGrad" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#fee2e2"/><stop offset="100%" style="stop-color:#f87171"/>
          </linearGradient>
        </defs>
        <!-- Query and Keys -->
        <rect x="50" y="150" width="60" height="40" fill="url(#decGrad)" stroke="#a855f7" stroke-width="2" rx="10"/>
        <text x="80" y="175" class="lbl" text-anchor="middle">q₍t₎</text>
        <g>
          <rect x="50" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="150" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="250" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <text x="80" y="65" class="lbl" text-anchor="middle">k₁</text>
          <text x="180" y="65" class="lbl" text-anchor="middle">k₂</text>
          <text x="280" y="65" class="lbl" text-anchor="middle">k₃</text>
        </g>
        <!-- Scoring operation -->
        <rect x="350" y="90" width="60" height="40" class="op"/>
        <text x="380" y="115" class="lbl" text-anchor="middle">q₍t₎·kᵢ</text>
        <!-- Softmax -->
        <rect x="450" y="90" width="60" height="40" fill="url(#scoreGrad)" stroke="#ef4444" stroke-width="2" rx="10"/>
        <text x="480" y="115" class="lbl" text-anchor="middle">softmax</text>
        <!-- Weights -->
        <rect x="550" y="90" width="60" height="40" class="op"/>
        <text x="580" y="115" class="lbl" text-anchor="middle">α₍t,i₎</text>
        <!-- Connections -->
        <g class="wire" marker-end="url(#arrow)">
          <path d="M 110 60 C 180 60 300 90 350 90"/>
          <path d="M 210 60 C 280 60 320 90 350 90"/>
          <path d="M 310 60 C 330 60 340 80 350 90"/>
          <path d="M 80 190 C 150 190 300 150 350 130"/>
          <path d="M 410 110 H 450"/>
          <path d="M 510 110 H 550"/>
        </g>
        <text x="380" y="180" class="tip" text-anchor="middle">Scores scaled by 1/√d, then normalized</text>
      </svg>
      <p class="tip mt-2">The dot product measures similarity between query and keys; softmax ensures weights sum to 1.</p>
    </article>
    <!-- Block 3: Context Vector Computation -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
      <h3 class="text-xl font-semibold mb-2">3. Context Vector Computation</h3>
      <p class="text-sm text-slate-700 mt-2">The attention weights \( \alpha_{t,i} \) are used to compute a weighted sum of values \( v_i \), producing the context vector \( c_t \).</p>
      <div class="text-sm text-slate-700 mt-2">
        <strong>Context Equation:</strong>
        \[ c_t = \sum_i \alpha_{t,i} v_i \]
      </div>
      <svg viewBox="0 0 700 250" class="w-full h-auto mt-4 tex2jax_ignore" xmlns="http://www.w3.org/2000/svg" role="img" aria-labelledby="attn-context-title attn-context-desc">
        <title id="attn-context-title">Context Vector Computation</title>
        <desc id="attn-context-desc">Diagram showing the weighted sum of values using attention weights to produce the context vector.</desc>
        <defs>
          <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
            <path d="M 0 0 L 10 5 L 0 10 z" fill="#334155"/>
          </marker>
        </defs>
        <!-- Values and Weights -->
        <g>
          <rect x="50" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="150" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="250" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <text x="80" y="65" class="lbl" text-anchor="middle">v₁</text>
          <text x="180" y="65" class="lbl" text-anchor="middle">v₂</text>
          <text x="280" y="65" class="lbl" text-anchor="middle">v₃</text>
        </g>
        <g>
          <rect x="50" y="150" width="60" height="40" fill="url(#scoreGrad)" stroke="#ef4444" stroke-width="2" rx="10"/>
          <rect x="150" y="150" width="60" height="40" fill="url(#scoreGrad)" stroke="#ef4444" stroke-width="2" rx="10"/>
          <rect x="250" y="150" width="60" height="40" fill="url(#scoreGrad)" stroke="#ef4444" stroke-width="2" rx="10"/>
          <text x="80" y="175" class="lbl" text-anchor="middle">α₍t,₁₎</text>
          <text x="180" y="175" class="lbl" text-anchor="middle">α₍t,₂₎</text>
          <text x="280" y="175" class="lbl" text-anchor="middle">α₍t,₃₎</text>
        </g>
        <!-- Weighted sum -->
        <circle cx="380" cy="100" r="20" class="add"/>
        <text x="380" y="105" class="lbl" text-anchor="middle">+</text>
        <!-- Context vector -->
        <rect x="500" y="80" width="60" height="40" class="op"/>
        <text x="530" y="105" class="lbl" text-anchor="middle">c₍t₎</text>
        <!-- Connections -->
        <g class="wire" marker-end="url(#arrow)">
          <path d="M 110 60 C 200 60 320 80 360 90"/>
          <path d="M 210 60 C 280 60 330 80 360 90"/>
          <path d="M 310 60 C 340 60 350 80 360 90"/>
          <path d="M 110 170 C 200 170 320 120 360 110"/>
          <path d="M 210 170 C 280 170 330 120 360 110"/>
          <path d="M 310 170 C 340 170 350 120 360 110"/>
          <path d="M 400 100 H 500"/>
        </g>
        <text x="380" y="180" class="tip" text-anchor="middle">Weighted sum of values</text>
      </svg>
      <p class="tip mt-2">The context vector \( c_t \) combines relevant information from encoder states, guided by attention weights.</p>
    </article>

      <!-- Block 4: Animated Attention Demonstration -->
    <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
      <h3 class="text-xl font-semibold mb-2">4. Animated Attention Demonstration</h3>
      <p class="text-sm text-slate-700 mt-2">Watch how attention flows: The query interacts with keys to compute scores, which are normalized via softmax to form weights, and then used to weight the values for the context vector.</p>
      <svg viewBox="0 0 600 400" class="w-full h-auto mt-4 tex2jax_ignore" xmlns="http://www.w3.org/2000/svg" role="img" aria-labelledby="attn-animated-title attn-animated-desc">
        <title id="attn-animated-title">Animated Attention Demonstration</title>
        <desc id="attn-animated-desc">Animated diagram demonstrating the flow of attention computation from query and keys to context vector.</desc>
        <defs>
          <marker id="arrow" viewBox="0 0 10 10" refX="9" refY="5" markerWidth="8" markerHeight="8" orient="auto-start-reverse">
            <path d="M 0 0 L 10 5 L 0 10 z" fill="#334155"/>
          </marker>
          <linearGradient id="encGrad" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#fef3c7"/><stop offset="100%" style="stop-color:#fde68a"/>
          </linearGradient>
          <linearGradient id="decGrad" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#e9d5ff"/><stop offset="100%" style="stop-color:#c4b5fd"/>
          </linearGradient>
          <linearGradient id="scoreGrad" x1="0%" y1="0%" x2="100%" y2="100%">
            <stop offset="0%" style="stop-color:#fee2e2"/><stop offset="100%" style="stop-color:#f87171"/>
          </linearGradient>
        </defs>
        <!-- Encoder states (keys/values) -->
        <g>
          <rect x="50" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="150" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <rect x="250" y="40" width="60" height="40" fill="url(#encGrad)" stroke="#f59e0b" stroke-width="2" rx="10"/>
          <text x="80" y="65" class="lbl" text-anchor="middle">k₁, v₁</text>
          <text x="180" y="65" class="lbl" text-anchor="middle">k₂, v₂</text>
          <text x="280" y="65" class="lbl" text-anchor="middle">k₃, v₃</text>
        </g>
        <!-- Query -->
        <rect x="150" y="200" width="60" height="40" fill="url(#decGrad)" stroke="#a855f7" stroke-width="2" rx="10"/>
        <text x="180" y="225" class="lbl" text-anchor="middle">q₍t₎</text>
        <!-- Scoring operation -->
        <rect x="350" y="80" width="60" height="40" class="op"/>
        <text x="380" y="105" class="lbl" text-anchor="middle">Scores</text>
        <!-- Softmax -->
        <rect x="450" y="80" width="60" height="40" fill="url(#scoreGrad)" stroke="#ef4444" stroke-width="2" rx="10"/>
        <text x="480" y="105" class="lbl" text-anchor="middle">softmax</text>
        <!-- Weights (animated opacity) -->
        <g opacity="0">
          <animate attributeName="opacity" values="0;1" dur="3s" repeatCount="indefinite" />
          <rect x="450" y="160" width="60" height="40" fill="url(#scoreGrad)" stroke="#ef4444" stroke-width="2" rx="10"/>
          <text x="480" y="185" class="lbl" text-anchor="middle">α₍t,i₎</text>
        </g>
        <!-- Weighted sum -->
        <circle cx="380" cy="280" r="20" class="add"/>
        <text x="380" y="285" class="lbl" text-anchor="middle">+</text>
        <!-- Context vector -->
        <rect x="500" y="260" width="60" height="40" class="op"/>
        <text x="530" y="285" class="lbl" text-anchor="middle">c₍t₎</text>
        <!-- Animated connections from keys to scores -->
        <g class="wire" marker-end="url(#arrow)">
          <path d="M 110 60 C 180 60 300 80 350 90" stroke-dasharray="5 5">
            <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" />
          </path>
          <path d="M 210 60 C 280 60 320 80 350 90" stroke-dasharray="5 5">
            <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" />
          </path>
          <path d="M 310 60 C 340 60 350 80 350 90" stroke-dasharray="5 5">
            <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" />
          </path>
        </g>
        <!-- Animated connection from query to scores -->
        <path class="wire" marker-end="url(#arrow)" d="M 180 240 C 250 240 300 200 350 130" stroke-dasharray="5 5">
          <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" />
        </path>
        <!-- Connection from scores to softmax -->
        <path class="wire" marker-end="url(#arrow)" d="M 410 100 H 450"/>
        <!-- Animated connection from softmax to weights -->
        <path class="wire" marker-end="url(#arrow)" d="M 510 100 V 180" stroke-dasharray="5 5">
          <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" begin="1s" />
        </path>
        <!-- Animated connections from weights and values to sum -->
        <g class="wire" marker-end="url(#arrow)">
          <path d="M 110 60 C 200 60 320 240 360 270" stroke-dasharray="5 5">
            <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" begin="2s" />
          </path>
          <path d="M 210 60 C 280 60 330 240 360 270" stroke-dasharray="5 5">
            <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" begin="2s" />
          </path>
          <path d="M 310 60 C 340 60 350 240 360 270" stroke-dasharray="5 5">
            <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" begin="2s" />
          </path>
          <path d="M 480 200 C 420 240 380 260 360 270" stroke-dasharray="5 5">
            <animate attributeName="stroke-dashoffset" values="10;0" dur="2s" repeatCount="indefinite" begin="2s" />
          </path>
        </g>
        <!-- Connection from sum to context -->
        <path class="wire" marker-end="url(#arrow)" d="M 400 280 H 500"/>
        <text x="300" y="350" class="tip" text-anchor="middle">Animation shows flow: Query/keys → Scores → Weights → Context</text>
      </svg>
      <p class="tip mt-2">The animation illustrates the sequential computation in attention, highlighting data flow over time.</p>
    </article>
    
  </div>
</section>


        <section id="attention" class="py-10 bg-slate-50">
          <div class="max-w-7xl mx-auto px-6 grid lg:grid-cols-2 gap-6">
            <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
              <h2 class="text-2xl font-bold">Encoder→Decoder with Attention</h2>
              <div class="text-sm text-slate-700 leading-7 mt-2">
                <b>Dot (Luong):</b> $\alpha_{t,i}=\operatorname{softmax}_i\big(\tfrac{q_t^\top k_i}{\sqrt{d}}\big)$,
                $c_t=\sum_i \alpha_{t,i} v_i$, $y_t=g([s_t;c_t])$.<br />
                <b>Additive (Bahdanau):</b> $e_{t,i}=v^\top\tanh(W_q q_t + W_k k_i)$,
                $\alpha=\operatorname{softmax}(e)$.
                <div class="mt-2">
                  <i>Masking for variable lengths:</i> set scores of pad positions to $-\infty$ before softmax:
                  \[
                  \alpha_{t}=\mathrm{softmax}\big(e_t + \log m\big),\quad
                  m_i=\begin{cases}0&\text{pad}\\1&\text{valid}\end{cases}
                  \]
                </div>
              </div>
              <h3 class="text-lg font-semibold mt-4">Tiny heatmap schematic</h3>
              <svg viewBox="0 0 360 160" class="w-full h-auto tex2jax_ignore">
                <defs>
                  <linearGradient id="gradA" x1="0" y1="0" x2="0" y2="1">
                    <stop offset="0%" stop-color="#fee2e2" />
                    <stop offset="100%" stop-color="#ef4444" />
                  </linearGradient>
                </defs>
                <!-- axes -->
                <text x="20" y="20" class="lbl">keys k₁…k₆</text>
                <text x="300" y="140" class="lbl" text-anchor="end">query qₜ</text>
                <!-- heat blocks -->
                <g transform="translate(20,30)">
                  <!-- 6 cells -->
                  <rect x="0" y="0" width="40" height="20" fill="#fee2e2" />
                  <rect x="40" y="0" width="40" height="20" fill="#fecaca" />
                  <rect x="80" y="0" width="40" height="20" fill="#fca5a5" />
                  <rect x="120" y="0" width="40" height="20" fill="#fb7185" />
                  <rect x="160" y="0" width="40" height="20" fill="url(#gradA)" />
                  <rect x="200" y="0" width="40" height="20" fill="#fee2e2" />
                </g>
                <!-- arrows combining -->
                <g class="wire" stroke="#64748b">
                  <path d="M40,70 C60,110 220,110 240,70" />
                  <path d="M80,70 C90,105 220,105 240,70" />
                  <path d="M160,70 C160,100 220,100 240,70" />
                </g>
                <rect x="250" y="55" width="70" height="30" class="op"></rect>
                <text x="285" y="74" class="lbl" text-anchor="middle">cₜ</text>
              </svg>
              <p class="tip mt-2">Line thickness ≈ weight; attention creates a content-based, step-wise read from
                encoder
                states.</p>
            </article>

            <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
              <h3 class="text-xl font-semibold">Training & Inference</h3>
              <h4 class="font-semibold mt-2">Teacher forcing vs free-running</h4>
              <ul class="text-sm text-slate-700 space-y-2 mt-1">
                <li><b>Teacher forcing</b>: feed $y_{t-1}$ (gold) during training → faster/steady.</li>
                <li><b>Exposure bias</b>: at test time you feed $\hat y_{t-1}$. Mitigate via <i>scheduled sampling</i>:
                  with prob $p_t$ use gold else prediction; anneal $p_t\downarrow$.</li>
              </ul>
              <pre class="text-sm code bg-slate-900 text-slate-100 p-4 rounded-xl overflow-x-auto"><code># Scheduled sampling
use_gold = torch.rand(()) &lt; p_t
inp_t = gold_{t-1} if use_gold else yhat_{t-1}
p_t *= 0.995  # or linear/cosine schedule per epoch</code></pre>

              <h4 class="font-semibold mt-4">Beam search (length norm)</h4>
              <pre class="text-sm code bg-slate-900 text-slate-100 p-4 rounded-xl overflow-x-auto"><code># score = (sum logp) / ( (5+L)^α / (5+1)^α ), α≈0.6
# keep top-K partial hypotheses; stop when all beams hit &lt;eos&gt; or max_len
</code></pre>

              <h4 class="font-semibold mt-4">Coverage/monotonicity (optional)</h4>
              <ul class="text-sm text-slate-700 space-y-1">
                <li>Coverage penalty: discourage repeatedly attending same positions.</li>
                <li>Monotonic or local attention for speech/TS: simplifies alignment & reduces quadratic cost.</li>
              </ul>
            </article>
          </div>
        </section>

        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h2 class="text-2xl font-bold">Encoder→Decoder with Attention</h2>
          <p class="text-sm text-slate-700 mt-2">Decoder forms a per-step weighted sum of encoder states via
            <em>q•K</em>,
            softmax weights, and V.</p>
          <img src="attention.svg">
          <div class="text-sm text-slate-700 leading-7 mt-3">
            \( \mathrm{Attn}(q,K,V)=\mathrm{softmax}\!\big(\tfrac{qK^\top}{\sqrt{d}}\big)V \) &nbsp; (shown as mini
            heatmap → weighted sum).
          </div>
          <p class="tip mt-2">Line thickness encodes attention weight; here the middle state (h₂) dominates.</p>
        </article>
  
      </div>
  </section>


  <!-- ======= Quick Recipes & Tips ======= -->
  <section id="recipes" class="py-10">
    <div class="max-w-7xl mx-auto px-6 grid md:grid-cols-3 gap-6">
      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h3 class="text-lg font-semibold flex items-center gap-2">
          <div class="w-4 h-4 bg-gradient-to-br from-blue-400 to-blue-600 rounded"></div>
          Seq Classification
        </h3>
        <ul class="mt-3 text-sm text-slate-700 space-y-2">
          <li><strong>Architecture:</strong> Embed → BiLSTM(128) → Pool(last/max) → Dense(ℓ2) → Dense(C) softmax</li>
          <li><strong>Training:</strong> AdamW 1e-3, dropout 0.3, grad clip 1.0</li>
          <li><strong>Data:</strong> Pad sequences, use attention pooling for variable lengths</li>
        </ul>
      </article>

      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h3 class="text-lg font-semibold flex items-center gap-2">
          <div class="w-4 h-4 bg-gradient-to-br from-green-400 to-green-600 rounded"></div>
          Seq2Seq (w/ Attn)
        </h3>
        <ul class="mt-3 text-sm text-slate-700 space-y-2">
          <li><strong>Architecture:</strong> 2×BiGRU encoder → GRU decoder with dot-product attention</li>
          <li><strong>Training:</strong> Teacher forcing, label smoothing 0.1, scheduled sampling</li>
          <li><strong>Inference:</strong> Beam search (width=4), length penalty</li>
        </ul>
      </article>

      <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
        <h3 class="text-lg font-semibold flex items-center gap-2">
          <div class="w-4 h-4 bg-gradient-to-br from-purple-400 to-purple-600 rounded"></div>
          Time Series Forecasting
        </h3>
        <ul class="mt-3 text-sm text-slate-700 space-y-2">
          <li><strong>Architecture:</strong> Windowed inputs → LSTM(64) → Dense(k) for k-step horizon</li>
          <li><strong>Training:</strong> RMSProp/Adam, early stopping on validation loss</li>
          <li><strong>Data:</strong> Normalize features, handle missing values, use sliding windows</li>
        </ul>
      </article>
    </div>
  </section>

  <!-- ======= Architecture Comparison ======= -->
  <section id="comparison" class="py-10 bg-slate-50">
    <div class="max-w-7xl mx-auto px-6">
      <h2 class="text-3xl font-bold text-center mb-8">Architecture Comparison</h2>
      <div class="grid md:grid-cols-3 gap-6">
        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h3 class="text-xl font-semibold text-blue-600 mb-4">LSTM</h3>
          <div class="space-y-3 text-sm text-slate-700">
            <div><strong>Memory:</strong> Separate cell state + hidden state</div>
            <div><strong>Gates:</strong> Input, Forget, Output + Candidate</div>
            <div><strong>Pros:</strong> Best for very long sequences, stable gradients</div>
            <div><strong>Cons:</strong> More parameters, slower training</div>
            <div><strong>Use when:</strong> Long-term dependencies critical</div>
          </div>
        </article>

        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h3 class="text-xl font-semibold text-green-600 mb-4">GRU</h3>
          <div class="space-y-3 text-sm text-slate-700">
            <div><strong>Memory:</strong> Single hidden state</div>
            <div><strong>Gates:</strong> Update, Reset + Candidate</div>
            <div><strong>Pros:</strong> Faster training, fewer parameters</div>
            <div><strong>Cons:</strong> May struggle with very long sequences</div>
            <div><strong>Use when:</strong> Moderate sequences, speed matters</div>
          </div>
        </article>

        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h3 class="text-xl font-semibold text-purple-600 mb-4">Attention</h3>
          <div class="space-y-3 text-sm text-slate-700">
            <div><strong>Memory:</strong> Access to all encoder states</div>
            <div><strong>Mechanism:</strong> Query-Key-Value attention</div>
            <div><strong>Pros:</strong> No information bottleneck, interpretable</div>
            <div><strong>Cons:</strong> Quadratic complexity in sequence length</div>
            <div><strong>Use when:</strong> Seq2seq tasks, alignment important</div>
          </div>
        </article>
      </div>
    </div>
  </section>

  <!-- ======= Common Pitfalls & Solutions ======= -->
  <section id="pitfalls" class="py-10">
    <div class="max-w-7xl mx-auto px-6">
      <h2 class="text-3xl font-bold text-center mb-8">Common Pitfalls & Solutions</h2>
      <div class="grid lg:grid-cols-2 gap-6">
        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h3 class="text-xl font-semibold text-red-600 mb-4">⚠️ Training Issues</h3>
          <div class="space-y-4">
            <div class="border-l-4 border-red-200 pl-4">
              <strong class="text-red-700">Vanishing Gradients</strong>
              <p class="text-sm text-slate-600 mt-1">Use LSTM/GRU, gradient clipping, skip connections, or residual
                connections</p>
            </div>
            <div class="border-l-4 border-red-200 pl-4">
              <strong class="text-red-700">Exploding Gradients</strong>
              <p class="text-sm text-slate-600 mt-1">Clip gradients to max norm 1.0-5.0, use gradient scaling</p>
            </div>
            <div class="border-l-4 border-red-200 pl-4">
              <strong class="text-red-700">Slow Convergence</strong>
              <p class="text-sm text-slate-600 mt-1">Try LayerNorm, better initialization (Xavier/He), learning rate
                scheduling</p>
            </div>
          </div>
        </article>

        <article class="bg-white border border-slate-200 rounded-2xl p-6 shadow card">
          <h3 class="text-xl font-semibold text-green-600 mb-4">✅ Best Practices</h3>
          <div class="space-y-4">
            <div class="border-l-4 border-green-200 pl-4">
              <strong class="text-green-700">Regularization</strong>
              <p class="text-sm text-slate-600 mt-1">Dropout (0.2-0.5), weight decay, early stopping on validation loss
              </p>
            </div>
            <div class="border-l-4 border-green-200 pl-4">
              <strong class="text-green-700">Data Preprocessing</strong>
              <p class="text-sm text-slate-600 mt-1">Normalize inputs, handle variable lengths with padding/masking</p>
            </div>
            <div class="border-l-4 border-green-200 pl-4">
              <strong class="text-green-700">Optimization</strong>
              <p class="text-sm text-slate-600 mt-1">Adam/AdamW with warm-up, cosine annealing, gradient accumulation
              </p>
            </div>
          </div>
        </article>
      </div>
    </div>
  </section>

  <!-- ======= Footer ======= -->
  <footer class="py-10 bg-slate-900 text-slate-200">
    <div class="max-w-7xl mx-auto px-6">
      <div class="flex flex-col md:flex-row items-center justify-between gap-4">
        <p class="text-sm">© <span id="year"></span> RNNs Cheatsheet — Comprehensive Visual Reference</p>
        <div class="text-xs text-slate-400">Tailwind • MathJax • Pure SVG • Responsive Design</div>
      </div>
      <div class="mt-4 pt-4 border-t border-slate-700 text-center text-xs text-slate-400">
        Covers LSTM, GRU, Encoder-Decoder, and Attention mechanisms with detailed diagrams and practical implementation
        tips
      </div>
    </div>
  </footer>

  <script>
    // Footer year
    document.addEventListener('DOMContentLoaded', () => {
      const y = document.getElementById('year');
      if (y) y.textContent = new Date().getFullYear();

      // Smooth scrolling for anchor links
      document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
          e.preventDefault();
          const target = document.querySelector(this.getAttribute('href'));
          if (target) {
            target.scrollIntoView({
              behavior: 'smooth',
              block: 'start'
            });
          }
        });
      });
    });
  </script>

</body>

</html>